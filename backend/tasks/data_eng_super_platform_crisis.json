{
  "task_id": "data_eng_super_platform_crisis",
  "name": "Data Platform Incident Triage and Recovery",
  "role": "data_engineer",
  "duration_minutes": 30,
  "scoring_hints": {
    "min_reading_time_seconds": 300
  },
  "test_runner": {
    "command": "pytest -q --tb=no",
    "working_dir": "/workspace/fintech-data-platform",
    "parse_pattern": "(?P<passed>\\d+)\\s+passed",
    "timeout_seconds": 90
  },
  "scenario": "You just joined as a senior data engineer at a Series B fintech (200 employees, $50M ARR). Day 3, and you've been pulled into an urgent situation.\n\n**The email from your VP of Engineering:**\n\n> Subject: Need your eyes on this ASAP\n>\n> We have three fires burning:\n>\n> 1. **Finance is blocked** — Month-end close is tomorrow. The revenue reconciliation job has been failing for 3 days. CFO is escalating to the CEO.\n>\n> 2. **Compliance audit in 2 weeks** — Auditors need complete transaction lineage from our payment processor to our warehouse. We currently can't prove data completeness.\n>\n> 3. **The platform is groaning** — Data volumes grew 10x this quarter. Jobs that took 5 minutes now take 2 hours. The team has been firefighting instead of fixing root causes.\n>\n> I need you to: (1) assess the situation, (2) propose a plan, and (3) start fixing what you can. The repo has our current pipeline code and some diagnostic artifacts.\n>\n> The previous senior DE left a month ago. Documentation is sparse. The team is junior (2 people, <1 year experience each). You're the most experienced person who's touched this system.\n>\n> Don't try to fix everything. Tell me what to fix first, what can wait, and what we need to stop doing.\n\n**Your mission:**\nExplore the codebase. Understand what's broken and why. Propose a prioritized plan. Implement the highest-impact fixes you can in the time available. Document your reasoning.\n\n**What you'll be evaluated on:**\n- How you explore and understand the system before acting\n- How you prioritize competing demands\n- The quality of your technical judgment and tradeoffs\n- How you validate your changes actually work\n- How clearly you communicate your reasoning and plan",

  "repo_structure": {
    "name": "fintech-data-platform",
    "files": {
      "README.md": "# Fintech Data Platform\n\nCore data infrastructure for Payments Co.\n\n## Architecture\n```\nPayment Processor API → Ingestion → Raw Layer (S3) → Transform → Warehouse (Postgres) → BI Tools\n```\n\n## Key Pipelines\n- `transaction_sync` — Pulls from payment processor, loads to warehouse\n- `revenue_reconciliation` — Matches transactions to invoices for finance\n- `compliance_export` — Generates audit reports\n\n## Team\n- 2 junior DEs (both <1 year)\n- Previous senior left 4 weeks ago\n- No on-call rotation (everyone firefights)\n\n## Known Issues\n- Revenue job failing (started 3 days ago)\n- Compliance job incomplete\n- Everything is slow\n\n## Running\n```\npytest -q  # Most tests currently failing\n```",

      "ARCHITECTURE.md": "# Architecture Notes\n\n(Found in previous engineer's notes folder)\n\n## Data Flow\n\n1. **Ingestion** (`pipeline/ingest.py`)\n   - Pulls from Payment Processor REST API\n   - Incremental by `updated_at` timestamp\n   - Stores raw JSON in S3, then loads to Postgres\n\n2. **Transformation** (`pipeline/transform.py`)\n   - Joins transactions with customer data\n   - Calculates derived fields (fees, net_amount)\n   - Feeds downstream reporting\n\n3. **Reconciliation** (`pipeline/reconciliation.py`)\n   - Matches warehouse transactions to invoice system\n   - Generates discrepancy reports for finance\n   - CRITICAL for month-end close\n\n## Concerns I never got to fix (from prev engineer)\n- [ ] No idempotency — reruns create duplicates\n- [ ] No schema evolution — API changes break us\n- [ ] No data quality checks — garbage in, garbage out\n- [ ] Incremental logic assumes API returns complete windows (it doesn't always)\n- [ ] No monitoring — we find out about failures from angry emails\n- [ ] Raw layer and warehouse can drift apart\n\n## Passwords / Secrets\nAll in environment variables. Ask DevOps.",

      "diagnostics/revenue_job_errors.log": "2024-01-12 02:15:33 ERROR [revenue_reconciliation] Job failed\n2024-01-12 02:15:33 ERROR [revenue_reconciliation] psycopg2.errors.UndefinedColumn: column \"payment_method\" does not exist\n2024-01-12 02:15:33 ERROR [revenue_reconciliation] Query: SELECT transaction_id, amount, payment_method, processed_at FROM transactions WHERE ...\n2024-01-12 02:15:33 ERROR [revenue_reconciliation] Hint: The \"payment_method\" column was added to the API response on 2024-01-09\n\n2024-01-13 02:15:41 ERROR [revenue_reconciliation] Job failed (same error)\n2024-01-14 02:15:38 ERROR [revenue_reconciliation] Job failed (same error)\n\n--- Manual investigation notes (from junior DE) ---\nLooks like the payment processor added a new field. Not sure how to add it to our table.\nPrevious engineer handled schema stuff. Created ticket DATA-847 but nobody picked it up.",

      "diagnostics/compliance_gaps.sql": "-- Compliance audit prep query\n-- Run by external auditors 2024-01-10\n\n-- Check 1: Transaction completeness\nSELECT \n    DATE(processed_at) as txn_date,\n    COUNT(*) as warehouse_count,\n    (SELECT COUNT(*) FROM raw_api_responses WHERE DATE(fetched_at) = DATE(t.processed_at)) as raw_count\nFROM transactions t\nWHERE processed_at >= '2023-07-01'\nGROUP BY DATE(processed_at)\nORDER BY txn_date;\n\n-- RESULTS SHOWED:\n-- Multiple days where warehouse_count < raw_count\n-- Gap ranges from 0.1% to 3% per day\n-- Total gap: ~12,000 transactions over 6 months\n-- Auditor comment: \"Need to explain or remediate before sign-off\"\n\n-- Check 2: Duplicate detection\nSELECT transaction_id, COUNT(*) as occurrences\nFROM transactions\nGROUP BY transaction_id\nHAVING COUNT(*) > 1;\n\n-- RESULTS: 847 duplicate transaction_ids\n-- Auditor comment: \"This inflates revenue numbers. Critical finding.\"",

      "diagnostics/performance_baseline.md": "# Performance Observations\n\nCaptured 2024-01-08 after complaints from team\n\n## Job Durations (last 30 days)\n\n| Job | Dec 1 | Dec 15 | Jan 1 | Jan 8 |\n|-----|-------|--------|-------|-------|\n| transaction_sync | 4m | 8m | 25m | 2h 10m |\n| revenue_recon | 2m | 3m | 12m | FAILING |\n| compliance_export | 1m | 2m | 8m | 45m |\n\n## Data Volumes\n- Transactions table: 47M rows (was 8M in October)\n- Daily ingest: ~500K records (was ~50K)\n- No partitioning on transactions table\n- No indexes except primary key\n\n## Infrastructure\n- Single Postgres instance (db.m5.xlarge)\n- No read replicas\n- Same instance serves BI queries and ETL writes\n- S3 raw layer: 2TB, no lifecycle policies\n\n## What previous engineer said before leaving\n\"We need to partition by month and add indexes. Also should probably move BI to a replica. Created tickets but they're in the backlog.\"",

      "pipeline/__init__.py": "",

      "pipeline/config.py": "from dataclasses import dataclass\nfrom typing import Optional\nimport os\n\n@dataclass\nclass PipelineConfig:\n    # Database\n    db_host: str = os.getenv('DB_HOST', 'localhost')\n    db_name: str = os.getenv('DB_NAME', 'fintech_warehouse')\n    db_user: str = os.getenv('DB_USER', 'etl_user')\n    db_password: str = os.getenv('DB_PASSWORD', '')\n    \n    # API\n    api_base_url: str = os.getenv('PAYMENT_API_URL', 'https://api.paymentprocessor.com')\n    api_key: str = os.getenv('PAYMENT_API_KEY', '')\n    \n    # Storage\n    raw_bucket: str = os.getenv('RAW_BUCKET', 's3://fintech-raw-data')\n    \n    # Operational\n    batch_size: int = 10000\n    max_retries: int = 3\n    checkpoint_table: str = 'pipeline_checkpoints'",

      "pipeline/ingest.py": "\"\"\"Transaction ingestion from Payment Processor API.\"\"\"\n\nimport json\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass IngestResult:\n    records_fetched: int\n    records_loaded: int\n    duplicates_skipped: int\n    errors: List[str]\n\n\nclass TransactionIngestor:\n    \"\"\"Pulls transactions from payment processor and loads to warehouse.\"\"\"\n    \n    def __init__(self, api_client, db_client, s3_client, config):\n        self.api = api_client\n        self.db = db_client\n        self.s3 = s3_client\n        self.config = config\n    \n    def run_incremental(self, since: Optional[datetime] = None) -> IngestResult:\n        \"\"\"Incremental sync based on updated_at timestamp.\"\"\"\n        \n        # Get checkpoint\n        if since is None:\n            since = self._get_checkpoint()\n        \n        logger.info(f\"Starting incremental ingest from {since}\")\n        \n        # Fetch from API\n        # BUG: API sometimes returns incomplete windows under load\n        # BUG: No handling for API pagination edge cases\n        records = self.api.fetch_transactions(since=since)\n        \n        # Store raw to S3 (for audit trail)\n        raw_path = self._store_raw(records)\n        \n        # Load to warehouse\n        # BUG: No schema evolution - new columns cause failures\n        # BUG: No deduplication - reruns create duplicates\n        loaded = self._load_to_warehouse(records)\n        \n        # Update checkpoint\n        # BUG: Checkpoint updated even if load partially failed\n        if records:\n            latest = max(r['updated_at'] for r in records)\n            self._save_checkpoint(latest)\n        \n        return IngestResult(\n            records_fetched=len(records),\n            records_loaded=loaded,\n            duplicates_skipped=0,  # Not implemented\n            errors=[]\n        )\n    \n    def run_backfill(self, start_date: datetime, end_date: datetime) -> IngestResult:\n        \"\"\"Historical backfill for a date range.\"\"\"\n        \n        # BUG: Uses same checkpoint as incremental\n        # BUG: Not idempotent - creates duplicates on retry\n        # BUG: No progress tracking for resumability\n        \n        total_fetched = 0\n        total_loaded = 0\n        \n        current = start_date\n        while current <= end_date:\n            records = self.api.fetch_transactions(\n                since=current,\n                until=current + timedelta(days=1)\n            )\n            self._store_raw(records)\n            loaded = self._load_to_warehouse(records)\n            \n            total_fetched += len(records)\n            total_loaded += loaded\n            current += timedelta(days=1)\n        \n        return IngestResult(\n            records_fetched=total_fetched,\n            records_loaded=total_loaded,\n            duplicates_skipped=0,\n            errors=[]\n        )\n    \n    def _get_checkpoint(self) -> datetime:\n        \"\"\"Get last successful sync timestamp.\"\"\"\n        result = self.db.query(\n            f\"SELECT last_value FROM {self.config.checkpoint_table} WHERE pipeline = 'transaction_sync'\"\n        )\n        if result:\n            return datetime.fromisoformat(result[0]['last_value'])\n        return datetime.now() - timedelta(days=7)\n    \n    def _save_checkpoint(self, timestamp: datetime) -> None:\n        \"\"\"Save sync checkpoint.\"\"\"\n        self.db.execute(\n            f\"INSERT INTO {self.config.checkpoint_table} (pipeline, last_value, updated_at) \"\n            f\"VALUES ('transaction_sync', %s, NOW()) \"\n            f\"ON CONFLICT (pipeline) DO UPDATE SET last_value = %s, updated_at = NOW()\",\n            [timestamp.isoformat(), timestamp.isoformat()]\n        )\n    \n    def _store_raw(self, records: List[Dict]) -> str:\n        \"\"\"Store raw API response to S3.\"\"\"\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        path = f\"{self.config.raw_bucket}/transactions/{timestamp}.json\"\n        self.s3.put_object(path, json.dumps(records))\n        return path\n    \n    def _load_to_warehouse(self, records: List[Dict]) -> int:\n        \"\"\"Load records to warehouse transactions table.\"\"\"\n        if not records:\n            return 0\n        \n        # BUG: Assumes schema matches - fails if API adds columns\n        # BUG: Simple INSERT - creates duplicates\n        columns = records[0].keys()\n        placeholders = ', '.join(['%s'] * len(columns))\n        \n        for record in records:\n            values = [record.get(c) for c in columns]\n            self.db.execute(\n                f\"INSERT INTO transactions ({', '.join(columns)}) VALUES ({placeholders})\",\n                values\n            )\n        \n        return len(records)\n    \n    def get_current_schema(self) -> Dict[str, str]:\n        \"\"\"Get current warehouse table schema.\"\"\"\n        result = self.db.query(\n            \"SELECT column_name, data_type FROM information_schema.columns \"\n            \"WHERE table_name = 'transactions' ORDER BY ordinal_position\"\n        )\n        return {row['column_name']: row['data_type'] for row in result}\n\n\nclass PaymentProcessorAPI:\n    \"\"\"Client for Payment Processor REST API.\"\"\"\n    \n    def __init__(self, base_url: str, api_key: str):\n        self.base_url = base_url\n        self.api_key = api_key\n    \n    def fetch_transactions(self, since: datetime, until: Optional[datetime] = None) -> List[Dict]:\n        \"\"\"Fetch transactions from API.\n        \n        Note: API recently added new fields:\n        - payment_method (added 2024-01-09)\n        - risk_score (added 2024-01-05)\n        - metadata (added 2023-12-15)\n        \"\"\"\n        # Mock implementation - in production makes HTTP request\n        pass\n    \n    def get_schema(self) -> Dict[str, str]:\n        \"\"\"Get current API response schema.\"\"\"\n        return {\n            'transaction_id': 'string',\n            'amount': 'decimal',\n            'currency': 'string',\n            'status': 'string',\n            'customer_id': 'string',\n            'processed_at': 'timestamp',\n            'updated_at': 'timestamp',\n            'payment_method': 'string',   # Added 2024-01-09\n            'risk_score': 'float',         # Added 2024-01-05\n            'metadata': 'json',            # Added 2023-12-15\n        }",

      "pipeline/reconciliation.py": "\"\"\"Revenue reconciliation between warehouse and invoice system.\"\"\"\n\nimport logging\nfrom datetime import datetime, date\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass \nclass ReconciliationResult:\n    matched: int\n    warehouse_only: int\n    invoice_only: int\n    amount_mismatches: int\n    total_variance: float\n    report_path: str\n\n\nclass RevenueReconciliation:\n    \"\"\"Matches warehouse transactions to invoice system for finance.\"\"\"\n    \n    def __init__(self, db_client, invoice_client, config):\n        self.db = db_client\n        self.invoice = invoice_client\n        self.config = config\n    \n    def run_monthly(self, year: int, month: int) -> ReconciliationResult:\n        \"\"\"Run monthly reconciliation for finance close.\"\"\"\n        \n        logger.info(f\"Starting reconciliation for {year}-{month:02d}\")\n        \n        # Get warehouse transactions\n        # BUG: This query references payment_method which doesn't exist in table yet\n        warehouse_txns = self._get_warehouse_transactions(year, month)\n        \n        # Get invoice system transactions\n        invoice_txns = self._get_invoice_transactions(year, month)\n        \n        # Match and compare\n        result = self._reconcile(warehouse_txns, invoice_txns)\n        \n        # Generate report\n        report_path = self._generate_report(result, year, month)\n        \n        return result\n    \n    def _get_warehouse_transactions(self, year: int, month: int) -> List[Dict]:\n        \"\"\"Get transactions from warehouse for the month.\"\"\"\n        \n        # BUG: Query assumes payment_method column exists\n        # BUG: No handling for duplicates in warehouse\n        # BUG: Very slow on large tables (no index on processed_at)\n        query = \"\"\"\n            SELECT \n                transaction_id,\n                amount,\n                currency,\n                status,\n                payment_method,\n                processed_at,\n                customer_id\n            FROM transactions\n            WHERE EXTRACT(YEAR FROM processed_at) = %s\n              AND EXTRACT(MONTH FROM processed_at) = %s\n              AND status = 'completed'\n            ORDER BY processed_at\n        \"\"\"\n        return self.db.query(query, [year, month])\n    \n    def _get_invoice_transactions(self, year: int, month: int) -> List[Dict]:\n        \"\"\"Get transactions from invoice system.\"\"\"\n        return self.invoice.get_transactions(year, month)\n    \n    def _reconcile(self, warehouse: List[Dict], invoices: List[Dict]) -> ReconciliationResult:\n        \"\"\"Match warehouse transactions to invoices.\"\"\"\n        \n        warehouse_by_id = {t['transaction_id']: t for t in warehouse}\n        invoice_by_id = {t['transaction_id']: t for t in invoices}\n        \n        matched = 0\n        warehouse_only = 0\n        invoice_only = 0\n        amount_mismatches = 0\n        total_variance = 0.0\n        \n        # Check warehouse transactions\n        for txn_id, w_txn in warehouse_by_id.items():\n            if txn_id in invoice_by_id:\n                i_txn = invoice_by_id[txn_id]\n                if abs(w_txn['amount'] - i_txn['amount']) > 0.01:\n                    amount_mismatches += 1\n                    total_variance += w_txn['amount'] - i_txn['amount']\n                else:\n                    matched += 1\n            else:\n                warehouse_only += 1\n        \n        # Check for invoice-only\n        for txn_id in invoice_by_id:\n            if txn_id not in warehouse_by_id:\n                invoice_only += 1\n        \n        return ReconciliationResult(\n            matched=matched,\n            warehouse_only=warehouse_only,\n            invoice_only=invoice_only,\n            amount_mismatches=amount_mismatches,\n            total_variance=total_variance,\n            report_path=''\n        )\n    \n    def _generate_report(self, result: ReconciliationResult, year: int, month: int) -> str:\n        \"\"\"Generate reconciliation report for finance.\"\"\"\n        # Would generate PDF/Excel report\n        return f\"reports/reconciliation_{year}_{month:02d}.xlsx\"",

      "pipeline/schema.py": "\"\"\"Schema evolution utilities.\"\"\"\n\nfrom typing import Dict, List, Tuple\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# API type -> Postgres type mapping\nTYPE_MAPPING = {\n    'string': 'VARCHAR(255)',\n    'decimal': 'NUMERIC(18,2)',\n    'timestamp': 'TIMESTAMP',\n    'json': 'JSONB',\n    'boolean': 'BOOLEAN',\n    # BUG: Missing mappings for float, integer\n}\n\n\ndef map_type(api_type: str) -> str:\n    \"\"\"Map API type to Postgres type.\"\"\"\n    return TYPE_MAPPING.get(api_type, 'TEXT')\n\n\ndef detect_schema_changes(api_schema: Dict[str, str], db_schema: Dict[str, str]) -> List[Tuple[str, str]]:\n    \"\"\"Detect columns in API that don't exist in database.\"\"\"\n    missing = []\n    for col_name, col_type in api_schema.items():\n        if col_name not in db_schema:\n            missing.append((col_name, map_type(col_type)))\n    return missing\n\n\ndef apply_schema_changes(db_client, table: str, changes: List[Tuple[str, str]]) -> None:\n    \"\"\"Apply schema changes to database table.\"\"\"\n    for col_name, col_type in changes:\n        logger.info(f\"Adding column {col_name} ({col_type}) to {table}\")\n        db_client.execute(f\"ALTER TABLE {table} ADD COLUMN IF NOT EXISTS {col_name} {col_type}\")",

      "pipeline/quality.py": "\"\"\"Data quality checks.\n\nTODO: This module is mostly unimplemented.\nPrevious engineer started it but never finished.\n\"\"\"\n\nfrom typing import Dict, List, Any\nfrom dataclasses import dataclass\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass QualityResult:\n    passed: bool\n    checks_run: int\n    checks_passed: int\n    failures: List[str]\n\n\nclass DataQualityChecker:\n    \"\"\"Runs data quality checks.\"\"\"\n    \n    def __init__(self, db_client):\n        self.db = db_client\n    \n    def check_completeness(self, date_range) -> bool:\n        \"\"\"Check if all expected records are present.\"\"\"\n        # TODO: Implement\n        pass\n    \n    def check_duplicates(self, table: str, key_column: str) -> List[str]:\n        \"\"\"Find duplicate keys in table.\"\"\"\n        # TODO: Implement\n        pass\n    \n    def check_referential_integrity(self) -> bool:\n        \"\"\"Check foreign key relationships.\"\"\"\n        # TODO: Implement\n        pass\n    \n    def run_all_checks(self) -> QualityResult:\n        \"\"\"Run all quality checks.\"\"\"\n        # TODO: Implement\n        return QualityResult(\n            passed=True,  # Always returns true - not actually checking\n            checks_run=0,\n            checks_passed=0,\n            failures=[]\n        )",

      "tests/__init__.py": "",

      "tests/test_ingest.py": "\"\"\"Tests for transaction ingestion.\"\"\"\n\nimport pytest\nfrom datetime import datetime, timedelta\nfrom unittest.mock import Mock, MagicMock\nfrom pipeline.ingest import TransactionIngestor, IngestResult\n\n\nclass TestTransactionIngestor:\n    \n    @pytest.fixture\n    def mock_deps(self):\n        \"\"\"Create mock dependencies.\"\"\"\n        api = Mock()\n        db = Mock()\n        s3 = Mock()\n        config = Mock()\n        config.checkpoint_table = 'pipeline_checkpoints'\n        config.raw_bucket = 's3://test-bucket'\n        return api, db, s3, config\n    \n    def test_incremental_handles_new_columns(self, mock_deps):\n        \"\"\"New columns from API should not cause failures.\"\"\"\n        api, db, s3, config = mock_deps\n        \n        # API returns records with new column\n        api.fetch_transactions.return_value = [\n            {'transaction_id': 'TXN-1', 'amount': 100.0, 'payment_method': 'card'},\n        ]\n        api.get_schema.return_value = {\n            'transaction_id': 'string',\n            'amount': 'decimal',\n            'payment_method': 'string',\n        }\n        \n        # DB schema missing payment_method\n        db.query.side_effect = [\n            [{'last_value': (datetime.now() - timedelta(days=1)).isoformat()}],  # checkpoint\n            [{'column_name': 'transaction_id', 'data_type': 'varchar'},\n             {'column_name': 'amount', 'data_type': 'numeric'}],  # schema\n        ]\n        \n        ingestor = TransactionIngestor(api, db, s3, config)\n        \n        # Should not raise, should evolve schema\n        result = ingestor.run_incremental()\n        assert result.records_loaded == 1\n    \n    def test_incremental_deduplicates_on_key(self, mock_deps):\n        \"\"\"Duplicate transaction_ids should not create duplicate rows.\"\"\"\n        api, db, s3, config = mock_deps\n        \n        # API returns same transaction twice (can happen with pagination)\n        api.fetch_transactions.return_value = [\n            {'transaction_id': 'TXN-1', 'amount': 100.0, 'updated_at': '2024-01-10T10:00:00'},\n            {'transaction_id': 'TXN-1', 'amount': 150.0, 'updated_at': '2024-01-10T11:00:00'},\n        ]\n        \n        db.query.return_value = [{'last_value': '2024-01-09T00:00:00'}]\n        \n        ingestor = TransactionIngestor(api, db, s3, config)\n        result = ingestor.run_incremental()\n        \n        # Should only insert once (latest version)\n        assert result.records_loaded == 1\n        assert result.duplicates_skipped == 1\n    \n    def test_backfill_is_idempotent(self, mock_deps):\n        \"\"\"Running backfill twice should not create duplicates.\"\"\"\n        api, db, s3, config = mock_deps\n        \n        records = [\n            {'transaction_id': 'TXN-1', 'amount': 100.0},\n            {'transaction_id': 'TXN-2', 'amount': 200.0},\n        ]\n        api.fetch_transactions.return_value = records\n        \n        ingestor = TransactionIngestor(api, db, s3, config)\n        \n        # Run twice\n        ingestor.run_backfill(datetime(2024, 1, 1), datetime(2024, 1, 1))\n        ingestor.run_backfill(datetime(2024, 1, 1), datetime(2024, 1, 1))\n        \n        # Count inserts - should be 2, not 4\n        insert_calls = [c for c in db.execute.call_args_list if 'INSERT' in str(c)]\n        # With idempotent design, second run should upsert, not double-insert\n    \n    def test_backfill_does_not_move_incremental_checkpoint(self, mock_deps):\n        \"\"\"Backfill should use separate checkpoint from incremental.\"\"\"\n        api, db, s3, config = mock_deps\n        \n        api.fetch_transactions.return_value = []\n        db.query.return_value = [{'last_value': '2024-01-10T00:00:00'}]\n        \n        ingestor = TransactionIngestor(api, db, s3, config)\n        \n        # Record incremental checkpoint\n        original_checkpoint = ingestor._get_checkpoint()\n        \n        # Run backfill for historical data\n        ingestor.run_backfill(datetime(2023, 6, 1), datetime(2023, 6, 30))\n        \n        # Incremental checkpoint should be unchanged\n        # (This test will fail with current implementation)",

      "tests/test_reconciliation.py": "\"\"\"Tests for revenue reconciliation.\"\"\"\n\nimport pytest\nfrom unittest.mock import Mock\nfrom pipeline.reconciliation import RevenueReconciliation\n\n\nclass TestRevenueReconciliation:\n    \n    @pytest.fixture\n    def mock_deps(self):\n        db = Mock()\n        invoice = Mock()\n        config = Mock()\n        return db, invoice, config\n    \n    def test_handles_missing_columns_gracefully(self, mock_deps):\n        \"\"\"Should not fail if warehouse schema is behind API.\"\"\"\n        db, invoice, config = mock_deps\n        \n        # Simulate the payment_method column not existing\n        db.query.side_effect = Exception(\"column 'payment_method' does not exist\")\n        \n        recon = RevenueReconciliation(db, invoice, config)\n        \n        # Should handle gracefully, not crash\n        # Either skip the column or evolve schema first\n    \n    def test_reconciliation_handles_duplicates_in_warehouse(self, mock_deps):\n        \"\"\"Duplicates in warehouse should be flagged, not double-counted.\"\"\"\n        db, invoice, config = mock_deps\n        \n        # Warehouse has duplicate\n        db.query.return_value = [\n            {'transaction_id': 'TXN-1', 'amount': 100.0},\n            {'transaction_id': 'TXN-1', 'amount': 100.0},  # Duplicate\n            {'transaction_id': 'TXN-2', 'amount': 200.0},\n        ]\n        \n        invoice.get_transactions.return_value = [\n            {'transaction_id': 'TXN-1', 'amount': 100.0},\n            {'transaction_id': 'TXN-2', 'amount': 200.0},\n        ]\n        \n        recon = RevenueReconciliation(db, invoice, config)\n        result = recon.run_monthly(2024, 1)\n        \n        # Should flag the duplicate as a data quality issue\n        # Revenue should be 300, not 400",

      "tests/test_schema.py": "\"\"\"Tests for schema evolution.\"\"\"\n\nimport pytest\nfrom pipeline.schema import map_type, detect_schema_changes\n\n\nclass TestSchemaEvolution:\n    \n    def test_maps_float_type_correctly(self):\n        \"\"\"Float should map to appropriate Postgres type.\"\"\"\n        result = map_type('float')\n        assert result in ['DOUBLE PRECISION', 'FLOAT8', 'NUMERIC']\n        assert result != 'TEXT'  # Default fallback is wrong for float\n    \n    def test_maps_integer_type_correctly(self):\n        \"\"\"Integer should map to appropriate Postgres type.\"\"\"\n        result = map_type('integer')\n        assert result in ['INTEGER', 'INT', 'INT4', 'BIGINT']\n        assert result != 'TEXT'\n    \n    def test_detects_new_columns(self):\n        \"\"\"Should detect columns in API not in database.\"\"\"\n        api_schema = {\n            'transaction_id': 'string',\n            'amount': 'decimal',\n            'payment_method': 'string',  # New\n            'risk_score': 'float',        # New\n        }\n        db_schema = {\n            'transaction_id': 'varchar',\n            'amount': 'numeric',\n        }\n        \n        changes = detect_schema_changes(api_schema, db_schema)\n        \n        assert len(changes) == 2\n        assert ('payment_method', 'VARCHAR(255)') in changes\n        # risk_score should map to float type, not TEXT",

      "Makefile": "# Development helpers\n\n.PHONY: test lint fix\n\ntest:\n\tpytest -q\n\ntest-verbose:\n\tpytest -v\n\nlint:\n\truff check .\n\nfix:\n\truff check --fix .",

      "requirements.txt": "pytest>=7.0.0\npsycopg2-binary>=2.9.0\nboto3>=1.26.0\nruff>=0.1.0"
    }
  },

  "evaluation_rubric": {
    "situational_assessment": {
      "weight": 0.20,
      "criteria": {
        "excellent": "Thoroughly explores diagnostics first. Maps the three fires to root causes. Identifies that revenue job failure is a schema issue (quick fix), compliance gap is a data quality/duplicate issue (medium), performance is infrastructure (longer term). Articulates dependencies between issues.",
        "good": "Explores the situation. Identifies main issues. Some prioritization rationale.",
        "poor": "Jumps to coding without understanding. Misses the connections between issues."
      }
    },
    "prioritization_judgment": {
      "weight": 0.20,
      "criteria": {
        "excellent": "Correctly prioritizes: (1) Unblock finance - fix schema evolution so recon job runs, (2) Stop the bleeding - fix ingestion to prevent more duplicates, (3) Document for compliance - explain gaps, propose remediation. Explicitly deprioritizes performance (important but not urgent).",
        "good": "Reasonable priority order. May not articulate why.",
        "poor": "Works on low-impact items first. Gets lost in performance optimization while finance is blocked."
      }
    },
    "technical_design": {
      "weight": 0.20,
      "criteria": {
        "excellent": "Proposes coherent fixes: schema evolution before writes, UPSERT for idempotency, separate backfill checkpoint. Discusses tradeoffs (full reconciliation vs. incremental, UPSERT performance). Considers operational concerns (monitoring, alerting).",
        "good": "Reasonable technical approach. Some gaps in edge cases.",
        "poor": "Ad-hoc fixes that don't address root causes."
      }
    },
    "implementation_quality": {
      "weight": 0.20,
      "criteria": {
        "excellent": "Implements fixes that actually work. Tests pass. Code is production-quality (error handling, logging). Changes are minimal and focused.",
        "good": "Most fixes work. Some tests pass. Code is reasonable.",
        "poor": "Fixes don't work or break other things."
      }
    },
    "communication_clarity": {
      "weight": 0.20,
      "criteria": {
        "excellent": "Could hand this to the VP. Clear explanation of: what's broken, why, what's fixed, what remains. Proposes next steps. Documents decisions.",
        "good": "Reasonable explanation of work done.",
        "poor": "No clear communication. Would need extensive follow-up."
      }
    }
  },

  "expected_candidate_journey": {
    "first_5_minutes": [
      "Read README and ARCHITECTURE.md to understand system",
      "Review diagnostics folder to understand the three fires",
      "Identify that revenue job is failing due to missing payment_method column",
      "Notice compliance gaps are duplicates + missing records",
      "Recognize performance is infrastructure-related (deprioritize)"
    ],
    "minutes_5_to_15": [
      "Fix schema.py to add float->DOUBLE PRECISION and integer->INTEGER mappings",
      "Modify ingest.py to evolve schema before loading",
      "Run tests to verify schema fix",
      "Start on deduplication - change INSERT to UPSERT pattern"
    ],
    "minutes_15_to_25": [
      "Complete deduplication fix with proper key handling",
      "Separate backfill checkpoint from incremental checkpoint",
      "Run full test suite",
      "Address any failing tests"
    ],
    "final_5_minutes": [
      "Document what was fixed and what remains",
      "Write up for VP: status, next steps, risks",
      "Commit clean, reviewable changes"
    ]
  },

  "interviewer_signals": {
    "strong_positive": [
      "Reads diagnostics before touching code",
      "Explicitly prioritizes finance unblock over performance",
      "Asks clarifying questions (even to Claude) about constraints",
      "Tests incrementally rather than writing all code then testing",
      "Communicates tradeoffs in their approach",
      "Acknowledges what they're NOT fixing and why"
    ],
    "red_flags": [
      "Immediately starts optimizing performance (wrong priority)",
      "Doesn't read the error logs or diagnostic files",
      "Changes tests to make them pass",
      "No testing during implementation",
      "Can't explain what they fixed or why",
      "Ignores the compliance/audit context"
    ]
  }
}
