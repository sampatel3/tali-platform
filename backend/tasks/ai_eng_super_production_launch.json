{
  "task_id": "ai_eng_super_production_launch",
  "name": "AI Feature Production Readiness Assessment",
  "role": "ai_engineer",
  "duration_minutes": 30,
  "calibration_prompt": "You have 2 minutes. Ask Claude for help designing a safe prompt strategy for a tiny utility function. Show how you provide context, constraints, and acceptance criteria before requesting code.",
  "scoring_hints": {
    "min_reading_time_seconds": 300
  },
  "test_runner": {
    "command": "pytest -q --tb=no",
    "working_dir": "/workspace/customer-intelligence-ai",
    "parse_pattern": "(?P<passed>\\d+)\\s+passed",
    "timeout_seconds": 90
  },
  "scenario": "You're a senior AI engineer at a B2B SaaS company (500 employees, enterprise customers). The company is betting big on AI - the CEO announced \"AI-powered features in every product\" at the last all-hands.\n\n**The Slack message from your engineering director:**\n\n> @you - Need you to take over the Customer Intelligence feature. Launch was supposed to be 2 weeks ago but we're stuck.\n>\n> **What it does:** AI analyzes customer support tickets and generates insights for account managers - churn risk, upsell opportunities, sentiment trends.\n>\n> **The problem:** The prototype \"works\" in demos but nobody trusts it for production:\n> - Sales is nervous about wrong insights reaching customers\n> - Legal flagged potential liability if we give bad advice\n> - Infra is worried about costs (prototype burned $2K in one day of testing)\n> - QA doesn't know how to test it (\"how do you test AI?\")\n>\n> The previous engineer built a working prototype but left for another company. Code is in the repo. Some tests exist but most are commented out or skipped.\n>\n> I need you to:\n> 1. Assess what's actually there and what's risky\n> 2. Make it production-ready (or tell me what's blocking that)\n> 3. Propose a launch plan that Legal/Sales/Infra can sign off on\n>\n> We have enterprise customers. Mistakes here are career-limiting. But shipping nothing is also not an option - the CEO committed to this publicly.\n>\n> Take 30 minutes to assess and start fixing. Then tell me: can we launch this, and if so, how?\n\n**Your mission:**\nUnderstand the system. Identify the risks. Make it safer. Propose a credible path to production. Show your judgment, not just your coding.\n\n**What you'll be evaluated on:**\n- How you assess risk (not just technical risk - business risk)\n- How you balance shipping vs. safety\n- The quality of your production-readiness improvements\n- How you communicate to non-technical stakeholders\n- Whether your plan would actually work",

  "repo_structure": {
    "name": "customer-intelligence-ai",
    "files": {
      "README.md": "# Customer Intelligence AI\n\nAI-powered insights for account managers.\n\n## Features\n- Churn risk scoring\n- Upsell opportunity detection\n- Sentiment analysis\n- Automated insight summaries\n\n## Status: PROTOTYPE\n⚠️ Not production-ready. See RISKS.md.\n\n## Architecture\n```\nSupport Tickets (DB) → Analysis Pipeline → LLM (GPT-4) → Insights API → Account Manager Dashboard\n```\n\n## Running\n```\npytest -q  # Some tests skipped\npython -m intelligence.main --customer-id CUST-123\n```\n\n## Cost Warning\nPrototype used GPT-4 for everything. Daily cost estimate at full scale: $3,000-5,000.\nNeed to optimize before launch.",

      "RISKS.md": "# Known Risks and Concerns\n\n## Flagged by Legal (2024-01-08)\n- [ ] AI could give advice that causes customer to make bad business decisions\n- [ ] No audit trail of what AI said and when\n- [ ] GDPR implications if we process EU customer data through OpenAI\n- [ ] Need disclaimer language reviewed\n\n## Flagged by Sales (2024-01-10)\n- [ ] \"What if it says a customer is high churn risk and they see it?\"\n- [ ] \"Can we trust the upsell suggestions? One wrong suggestion could blow a deal.\"\n- [ ] \"Account managers need to be able to override/dismiss insights\"\n\n## Flagged by Security (2024-01-09)\n- [ ] PII in support tickets being sent to OpenAI\n- [ ] No data retention policy for generated insights\n- [ ] API keys in environment variables (fine) but also in one config file (not fine)\n\n## Flagged by Infra (2024-01-11)\n- [ ] $2,100 spent in one day during load testing\n- [ ] No rate limiting on insight generation\n- [ ] No caching - regenerates insights on every request\n- [ ] GPT-4 latency too high for dashboard (8-15 second responses)\n\n## Flagged by QA (2024-01-12)\n- [ ] \"How do we test if insights are correct?\"\n- [ ] \"What's the expected behavior? No spec.\"\n- [ ] \"Skipped tests say 'TODO: figure out how to test LLM outputs'\"\n\n## Previous Engineer's Notes (before leaving)\n\"This works for demos. Production needs:\n1. PII redaction before LLM calls\n2. Human review workflow for high-stakes insights\n3. Caching and cheaper model for low-risk analysis\n4. Evaluation framework - we have no idea if it's actually good\n5. Fallback when LLM is down or slow\n\nI started some of this but didn't finish. Good luck.\"",

      "architecture/design_doc.md": "# Customer Intelligence Design\n\n## Overview\nAnalyze support tickets to generate actionable insights for account managers.\n\n## Data Flow\n1. **Ingest**: Pull support tickets from Zendesk API\n2. **Enrich**: Add customer metadata (ARR, contract dates, past interactions)\n3. **Analyze**: Send to LLM for analysis\n4. **Store**: Save insights to database\n5. **Serve**: API for dashboard\n\n## Insight Types\n\n### Churn Risk (HIGH STAKES)\n- Predicts likelihood customer will churn\n- Based on: support volume, sentiment, contract timing\n- Risk: Wrong prediction could cause account manager to deprioritize or over-contact\n\n### Upsell Opportunity (MEDIUM STAKES)\n- Identifies potential upsell based on usage patterns\n- Based on: feature requests, usage growth, pain points\n- Risk: Bad suggestion wastes account manager time, annoys customer\n\n### Sentiment Summary (LOW STAKES)\n- Summarizes overall sentiment from recent tickets\n- Based on: ticket content, resolution time, CSAT scores\n- Risk: Low - this is informational only\n\n## Model Selection (TBD)\n- Current: GPT-4 for everything (expensive, slow, good quality)\n- Proposed: Tiered approach\n  - GPT-4: Churn risk (high stakes, needs best quality)\n  - GPT-3.5: Upsell, sentiment (lower stakes, faster, cheaper)\n  - Local model: PII detection, preprocessing\n\n## Open Questions\n- How do we evaluate insight quality?\n- What's the human-in-the-loop workflow?\n- How do we handle LLM failures gracefully?\n- What's the caching strategy?",

      "intelligence/__init__.py": "",

      "intelligence/config.py": "import os\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass Config:\n    # LLM\n    openai_api_key: str = os.getenv('OPENAI_API_KEY', '')\n    openai_model: str = 'gpt-4'  # TODO: Should vary by insight type\n    \n    # Database\n    db_url: str = os.getenv('DATABASE_URL', 'postgresql://localhost/intelligence')\n    \n    # Feature flags\n    enable_churn_prediction: bool = True\n    enable_upsell_detection: bool = True\n    enable_sentiment_analysis: bool = True\n    \n    # Safety\n    require_human_review: bool = False  # BUG: Should be True for high-stakes\n    pii_redaction_enabled: bool = False  # BUG: Should be True\n    \n    # Performance\n    cache_ttl_seconds: int = 3600\n    max_tokens_per_request: int = 2000\n    request_timeout_seconds: int = 30\n    \n    # Costs (per 1K tokens)\n    gpt4_input_cost: float = 0.03\n    gpt4_output_cost: float = 0.06\n    gpt35_input_cost: float = 0.001\n    gpt35_output_cost: float = 0.002\n\n# BUG: API key also hardcoded here from early testing\n# OPENAI_API_KEY = 'sk-proj-xxxxxxxxxxxxx'  # REMOVED but was here",

      "intelligence/analyzer.py": "\"\"\"Core analysis logic for customer intelligence.\"\"\"\n\nimport json\nimport logging\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\nfrom intelligence.config import Config\nfrom intelligence.llm import LLMClient\nfrom intelligence.prompts import CHURN_PROMPT, UPSELL_PROMPT, SENTIMENT_PROMPT\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass Insight:\n    insight_type: str  # 'churn_risk', 'upsell', 'sentiment'\n    customer_id: str\n    summary: str\n    confidence: float  # 0-1\n    supporting_evidence: List[str]\n    recommended_action: Optional[str]\n    generated_at: datetime\n    model_used: str\n    tokens_used: int\n    \n    def to_dict(self) -> Dict:\n        return {\n            'insight_type': self.insight_type,\n            'customer_id': self.customer_id,\n            'summary': self.summary,\n            'confidence': self.confidence,\n            'evidence': self.supporting_evidence,\n            'action': self.recommended_action,\n            'generated_at': self.generated_at.isoformat(),\n            'model': self.model_used,\n        }\n\n\nclass CustomerAnalyzer:\n    \"\"\"Analyzes customer data to generate insights.\"\"\"\n    \n    def __init__(self, config: Config, llm_client: LLMClient, db_client):\n        self.config = config\n        self.llm = llm_client\n        self.db = db_client\n    \n    def analyze_customer(self, customer_id: str) -> List[Insight]:\n        \"\"\"Run all enabled analyses for a customer.\"\"\"\n        \n        # Get customer data\n        customer = self._get_customer_data(customer_id)\n        tickets = self._get_recent_tickets(customer_id)\n        \n        insights = []\n        \n        # BUG: No PII redaction before sending to LLM\n        # BUG: No caching - regenerates every time\n        # BUG: No error handling if LLM fails\n        # BUG: Uses GPT-4 for everything (expensive)\n        \n        if self.config.enable_churn_prediction:\n            insight = self._analyze_churn_risk(customer, tickets)\n            insights.append(insight)\n        \n        if self.config.enable_upsell_detection:\n            insight = self._analyze_upsell_opportunity(customer, tickets)\n            insights.append(insight)\n        \n        if self.config.enable_sentiment_analysis:\n            insight = self._analyze_sentiment(customer, tickets)\n            insights.append(insight)\n        \n        # Store insights\n        # BUG: No audit trail of what was generated\n        self._store_insights(insights)\n        \n        return insights\n    \n    def _analyze_churn_risk(self, customer: Dict, tickets: List[Dict]) -> Insight:\n        \"\"\"Analyze churn risk for customer.\"\"\"\n        \n        # Prepare context\n        context = self._prepare_context(customer, tickets)\n        \n        # BUG: Sends full context including PII (names, emails, etc.)\n        prompt = CHURN_PROMPT.format(\n            customer_name=customer['name'],\n            arr=customer['arr'],\n            contract_end=customer['contract_end_date'],\n            ticket_summary=context\n        )\n        \n        # BUG: No fallback if LLM fails\n        # BUG: No timeout handling\n        response = self.llm.complete(prompt)\n        \n        # Parse response\n        # BUG: Assumes LLM always returns valid JSON\n        # BUG: No validation of confidence score range\n        result = json.loads(response.content)\n        \n        return Insight(\n            insight_type='churn_risk',\n            customer_id=customer['id'],\n            summary=result.get('summary', 'Unable to analyze'),\n            confidence=result.get('confidence', 0.5),\n            supporting_evidence=result.get('evidence', []),\n            recommended_action=result.get('action'),\n            generated_at=datetime.now(),\n            model_used=self.config.openai_model,\n            tokens_used=response.tokens_used\n        )\n    \n    def _analyze_upsell_opportunity(self, customer: Dict, tickets: List[Dict]) -> Insight:\n        \"\"\"Analyze upsell opportunities.\"\"\"\n        # Similar to churn but with UPSELL_PROMPT\n        context = self._prepare_context(customer, tickets)\n        \n        prompt = UPSELL_PROMPT.format(\n            customer_name=customer['name'],\n            current_plan=customer.get('plan', 'unknown'),\n            usage_data=customer.get('usage', {}),\n            ticket_summary=context\n        )\n        \n        response = self.llm.complete(prompt)\n        result = json.loads(response.content)\n        \n        return Insight(\n            insight_type='upsell',\n            customer_id=customer['id'],\n            summary=result.get('summary', 'Unable to analyze'),\n            confidence=result.get('confidence', 0.5),\n            supporting_evidence=result.get('evidence', []),\n            recommended_action=result.get('action'),\n            generated_at=datetime.now(),\n            model_used=self.config.openai_model,\n            tokens_used=response.tokens_used\n        )\n    \n    def _analyze_sentiment(self, customer: Dict, tickets: List[Dict]) -> Insight:\n        \"\"\"Analyze overall sentiment.\"\"\"\n        context = self._prepare_context(customer, tickets)\n        \n        prompt = SENTIMENT_PROMPT.format(\n            customer_name=customer['name'],\n            ticket_summary=context\n        )\n        \n        response = self.llm.complete(prompt)\n        result = json.loads(response.content)\n        \n        return Insight(\n            insight_type='sentiment',\n            customer_id=customer['id'],\n            summary=result.get('summary', 'Unable to analyze'),\n            confidence=result.get('confidence', 0.5),\n            supporting_evidence=result.get('evidence', []),\n            recommended_action=None,  # Sentiment doesn't have actions\n            generated_at=datetime.now(),\n            model_used=self.config.openai_model,\n            tokens_used=response.tokens_used\n        )\n    \n    def _prepare_context(self, customer: Dict, tickets: List[Dict]) -> str:\n        \"\"\"Prepare context string for LLM.\"\"\"\n        # BUG: Includes raw ticket content with PII\n        lines = []\n        for ticket in tickets[:10]:  # Last 10 tickets\n            lines.append(f\"[{ticket['created_at']}] {ticket['subject']}: {ticket['content']}\")\n        return \"\\n\".join(lines)\n    \n    def _get_customer_data(self, customer_id: str) -> Dict:\n        \"\"\"Get customer metadata from database.\"\"\"\n        return self.db.query(\n            \"SELECT * FROM customers WHERE id = %s\",\n            [customer_id]\n        )[0]\n    \n    def _get_recent_tickets(self, customer_id: str) -> List[Dict]:\n        \"\"\"Get recent support tickets for customer.\"\"\"\n        return self.db.query(\n            \"SELECT * FROM tickets WHERE customer_id = %s ORDER BY created_at DESC LIMIT 20\",\n            [customer_id]\n        )\n    \n    def _store_insights(self, insights: List[Insight]) -> None:\n        \"\"\"Store generated insights.\"\"\"\n        for insight in insights:\n            self.db.execute(\n                \"INSERT INTO insights (type, customer_id, content, confidence, created_at) VALUES (%s, %s, %s, %s, %s)\",\n                [insight.insight_type, insight.customer_id, json.dumps(insight.to_dict()), insight.confidence, insight.generated_at]\n            )",

      "intelligence/llm.py": "\"\"\"LLM client wrapper.\"\"\"\n\nimport openai\nimport logging\nfrom typing import Optional\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass LLMResponse:\n    content: str\n    tokens_used: int\n    model: str\n    latency_ms: float\n\n\nclass LLMClient:\n    \"\"\"Wrapper for LLM API calls.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = 'gpt-4'):\n        self.client = openai.OpenAI(api_key=api_key)\n        self.model = model\n        self.total_tokens = 0\n        self.total_cost = 0.0\n    \n    def complete(self, prompt: str, max_tokens: int = 1000) -> LLMResponse:\n        \"\"\"Send completion request.\"\"\"\n        \n        # BUG: No retry logic\n        # BUG: No timeout\n        # BUG: No rate limiting\n        # BUG: No fallback model\n        \n        import time\n        start = time.time()\n        \n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=max_tokens,\n            temperature=0.3  # Lower for more consistent outputs\n        )\n        \n        latency = (time.time() - start) * 1000\n        tokens = response.usage.total_tokens\n        \n        self.total_tokens += tokens\n        # Cost calculation (simplified)\n        self.total_cost += tokens * 0.00003  # Rough GPT-4 cost\n        \n        logger.info(f\"LLM call: {self.model}, tokens={tokens}, latency={latency:.0f}ms\")\n        \n        return LLMResponse(\n            content=response.choices[0].message.content,\n            tokens_used=tokens,\n            model=self.model,\n            latency_ms=latency\n        )\n    \n    def get_usage_stats(self):\n        return {\n            'total_tokens': self.total_tokens,\n            'estimated_cost_usd': self.total_cost\n        }",

      "intelligence/prompts.py": "\"\"\"Prompt templates for analysis.\"\"\"\n\nCHURN_PROMPT = \"\"\"\nYou are an expert customer success analyst. Analyze the following customer data and recent support interactions to assess churn risk.\n\nCustomer: {customer_name}\nAnnual Revenue: ${arr}\nContract End Date: {contract_end}\n\nRecent Support Tickets:\n{ticket_summary}\n\nProvide your analysis as JSON with the following structure:\n{{\n    \"summary\": \"Brief summary of churn risk assessment\",\n    \"confidence\": 0.0-1.0,\n    \"risk_level\": \"low|medium|high|critical\",\n    \"evidence\": [\"list of supporting evidence from tickets\"],\n    \"action\": \"Recommended action for account manager\"\n}}\n\nBe specific and actionable. Base your assessment only on the provided data.\n\"\"\"\n\nUPSELL_PROMPT = \"\"\"\nYou are an expert sales analyst. Analyze the following customer data to identify upsell opportunities.\n\nCustomer: {customer_name}\nCurrent Plan: {current_plan}\nUsage Data: {usage_data}\n\nRecent Support Tickets:\n{ticket_summary}\n\nProvide your analysis as JSON with the following structure:\n{{\n    \"summary\": \"Brief summary of upsell opportunity\",\n    \"confidence\": 0.0-1.0,\n    \"opportunity_type\": \"expansion|upgrade|add-on|none\",\n    \"evidence\": [\"list of supporting evidence\"],\n    \"action\": \"Specific upsell recommendation\"\n}}\n\nOnly suggest upsells that are clearly supported by the data.\n\"\"\"\n\nSENTIMENT_PROMPT = \"\"\"\nYou are a customer experience analyst. Analyze the following support tickets to summarize overall customer sentiment.\n\nCustomer: {customer_name}\n\nRecent Support Tickets:\n{ticket_summary}\n\nProvide your analysis as JSON with the following structure:\n{{\n    \"summary\": \"Brief summary of overall sentiment\",\n    \"confidence\": 0.0-1.0,\n    \"sentiment\": \"positive|neutral|negative|mixed\",\n    \"evidence\": [\"list of key sentiment indicators\"],\n    \"trend\": \"improving|stable|declining\"\n}}\n\nFocus on factual observations, not assumptions.\n\"\"\"",

      "intelligence/safety.py": "\"\"\"Safety and PII handling.\n\nTODO: This module is not fully implemented.\nPrevious engineer started but didn't finish.\n\"\"\"\n\nimport re\nfrom typing import List, Tuple\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Common PII patterns\nEMAIL_PATTERN = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\nPHONE_PATTERN = r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'\nSSN_PATTERN = r'\\b\\d{3}-\\d{2}-\\d{4}\\b'\nCREDIT_CARD_PATTERN = r'\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b'\n\n\ndef redact_pii(text: str) -> Tuple[str, List[str]]:\n    \"\"\"Redact PII from text.\n    \n    Returns:\n        Tuple of (redacted_text, list of redaction types applied)\n    \"\"\"\n    redactions = []\n    \n    # Email\n    if re.search(EMAIL_PATTERN, text):\n        text = re.sub(EMAIL_PATTERN, '[EMAIL_REDACTED]', text)\n        redactions.append('email')\n    \n    # Phone\n    if re.search(PHONE_PATTERN, text):\n        text = re.sub(PHONE_PATTERN, '[PHONE_REDACTED]', text)\n        redactions.append('phone')\n    \n    # SSN\n    if re.search(SSN_PATTERN, text):\n        text = re.sub(SSN_PATTERN, '[SSN_REDACTED]', text)\n        redactions.append('ssn')\n    \n    # Credit card\n    if re.search(CREDIT_CARD_PATTERN, text):\n        text = re.sub(CREDIT_CARD_PATTERN, '[CC_REDACTED]', text)\n        redactions.append('credit_card')\n    \n    # TODO: Name detection (harder - needs NER)\n    # TODO: Address detection\n    # TODO: Custom patterns for business-specific PII\n    \n    return text, redactions\n\n\ndef validate_insight_safety(insight: dict) -> List[str]:\n    \"\"\"Check if insight content is safe to show.\n    \n    Returns list of safety concerns (empty if safe).\n    \"\"\"\n    concerns = []\n    \n    # TODO: Implement safety checks\n    # - No specific medical/legal/financial advice\n    # - No discriminatory language\n    # - No competitor mentions\n    # - Confidence thresholds for different insight types\n    \n    return concerns\n\n\ndef should_require_human_review(insight: dict) -> bool:\n    \"\"\"Determine if insight needs human review before display.\"\"\"\n    \n    # TODO: Implement review logic\n    # - High-stakes insights (churn risk > 0.7)\n    # - Low confidence (< 0.5)\n    # - Contains recommended actions\n    \n    return False  # Currently always returns False - bug!",

      "intelligence/cache.py": "\"\"\"Caching for insights.\n\nTODO: Not implemented. All calls go directly to LLM.\n\"\"\"\n\nfrom typing import Optional, Any\nimport json\nimport hashlib\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass InsightCache:\n    \"\"\"Cache for generated insights.\"\"\"\n    \n    def __init__(self, redis_client=None, ttl_seconds: int = 3600):\n        self.redis = redis_client\n        self.ttl = ttl_seconds\n    \n    def get(self, customer_id: str, insight_type: str) -> Optional[dict]:\n        \"\"\"Get cached insight if available.\"\"\"\n        # TODO: Implement\n        return None\n    \n    def set(self, customer_id: str, insight_type: str, insight: dict) -> None:\n        \"\"\"Cache an insight.\"\"\"\n        # TODO: Implement\n        pass\n    \n    def invalidate(self, customer_id: str) -> None:\n        \"\"\"Invalidate all cached insights for a customer.\"\"\"\n        # TODO: Implement\n        pass\n    \n    def _cache_key(self, customer_id: str, insight_type: str) -> str:\n        return f\"insight:{customer_id}:{insight_type}\"",

      "tests/__init__.py": "",

      "tests/test_analyzer.py": "\"\"\"Tests for customer analyzer.\"\"\"\n\nimport pytest\nfrom unittest.mock import Mock, MagicMock, patch\nimport json\n\nfrom intelligence.analyzer import CustomerAnalyzer, Insight\nfrom intelligence.config import Config\n\n\nclass TestCustomerAnalyzer:\n    \n    @pytest.fixture\n    def mock_deps(self):\n        config = Config()\n        llm = Mock()\n        db = Mock()\n        return config, llm, db\n    \n    def test_redacts_pii_before_llm_call(self, mock_deps):\n        \"\"\"PII should be redacted before sending to LLM.\"\"\"\n        config, llm, db = mock_deps\n        \n        # Customer data with PII\n        db.query.side_effect = [\n            [{'id': 'CUST-1', 'name': 'Acme Corp', 'arr': 50000, 'contract_end_date': '2024-06-01'}],\n            [{'content': 'Please contact john.doe@acme.com or call 555-123-4567', 'subject': 'Help', 'created_at': '2024-01-01'}]\n        ]\n        \n        llm.complete.return_value = Mock(\n            content=json.dumps({'summary': 'test', 'confidence': 0.5, 'evidence': [], 'action': None}),\n            tokens_used=100\n        )\n        \n        analyzer = CustomerAnalyzer(config, llm, db)\n        analyzer._analyze_churn_risk(\n            {'id': 'CUST-1', 'name': 'Acme', 'arr': 50000, 'contract_end_date': '2024-06-01'},\n            [{'content': 'Email me at john@acme.com', 'subject': 'Help', 'created_at': '2024-01-01'}]\n        )\n        \n        # Check that the prompt sent to LLM doesn't contain raw email\n        prompt_sent = llm.complete.call_args[0][0]\n        assert 'john@acme.com' not in prompt_sent, \"PII was not redacted!\"\n        assert '@' not in prompt_sent or 'REDACTED' in prompt_sent\n    \n    @pytest.mark.skip(reason=\"TODO: Figure out how to test LLM output quality\")\n    def test_churn_prediction_accuracy(self):\n        \"\"\"Churn predictions should be reasonably accurate.\"\"\"\n        # How do we even test this?\n        pass\n    \n    def test_handles_llm_failure_gracefully(self, mock_deps):\n        \"\"\"Should not crash if LLM call fails.\"\"\"\n        config, llm, db = mock_deps\n        \n        db.query.side_effect = [\n            [{'id': 'CUST-1', 'name': 'Acme', 'arr': 50000, 'contract_end_date': '2024-06-01'}],\n            []\n        ]\n        \n        # LLM throws error\n        llm.complete.side_effect = Exception(\"API rate limit exceeded\")\n        \n        analyzer = CustomerAnalyzer(config, llm, db)\n        \n        # Should handle gracefully, not crash\n        # Should return some fallback insight or error indicator\n    \n    def test_high_stakes_insights_require_review(self, mock_deps):\n        \"\"\"High confidence churn risk should require human review.\"\"\"\n        config, llm, db = mock_deps\n        config.require_human_review = True\n        \n        db.query.side_effect = [\n            [{'id': 'CUST-1', 'name': 'Acme', 'arr': 50000, 'contract_end_date': '2024-06-01'}],\n            []\n        ]\n        \n        # LLM returns high-risk prediction\n        llm.complete.return_value = Mock(\n            content=json.dumps({\n                'summary': 'High churn risk',\n                'confidence': 0.9,\n                'risk_level': 'critical',\n                'evidence': ['Multiple escalations'],\n                'action': 'Immediate executive outreach'\n            }),\n            tokens_used=100\n        )\n        \n        analyzer = CustomerAnalyzer(config, llm, db)\n        insights = analyzer.analyze_customer('CUST-1')\n        \n        # High-stakes insight should be flagged for review\n        churn_insight = next(i for i in insights if i.insight_type == 'churn_risk')\n        # assert churn_insight.requires_review == True\n    \n    def test_caches_insights_to_reduce_cost(self, mock_deps):\n        \"\"\"Repeated requests should use cache, not LLM.\"\"\"\n        config, llm, db = mock_deps\n        \n        db.query.side_effect = [\n            [{'id': 'CUST-1', 'name': 'Acme', 'arr': 50000, 'contract_end_date': '2024-06-01'}],\n            [],\n        ] * 2  # For two calls\n        \n        llm.complete.return_value = Mock(\n            content=json.dumps({'summary': 'test', 'confidence': 0.5, 'evidence': [], 'action': None}),\n            tokens_used=100\n        )\n        \n        analyzer = CustomerAnalyzer(config, llm, db)\n        \n        # First call\n        analyzer.analyze_customer('CUST-1')\n        first_call_count = llm.complete.call_count\n        \n        # Second call within cache TTL\n        analyzer.analyze_customer('CUST-1')\n        second_call_count = llm.complete.call_count\n        \n        # Second call should not increase LLM calls (use cache)\n        # assert second_call_count == first_call_count, \"Cache not working - LLM called again\"",

      "tests/test_safety.py": "\"\"\"Tests for safety module.\"\"\"\n\nimport pytest\nfrom intelligence.safety import redact_pii, validate_insight_safety, should_require_human_review\n\n\nclass TestPIIRedaction:\n    \n    def test_redacts_email(self):\n        text = \"Contact john.doe@company.com for help\"\n        redacted, types = redact_pii(text)\n        \n        assert 'john.doe@company.com' not in redacted\n        assert '[EMAIL_REDACTED]' in redacted\n        assert 'email' in types\n    \n    def test_redacts_phone(self):\n        text = \"Call us at 555-123-4567\"\n        redacted, types = redact_pii(text)\n        \n        assert '555-123-4567' not in redacted\n        assert 'phone' in types\n    \n    def test_redacts_ssn(self):\n        text = \"SSN: 123-45-6789\"\n        redacted, types = redact_pii(text)\n        \n        assert '123-45-6789' not in redacted\n        assert 'ssn' in types\n    \n    def test_redacts_credit_card(self):\n        text = \"Card: 4111-1111-1111-1111\"\n        redacted, types = redact_pii(text)\n        \n        assert '4111' not in redacted\n        assert 'credit_card' in types\n    \n    def test_preserves_non_pii(self):\n        text = \"Customer ID: CUST-12345, Order: ORD-67890\"\n        redacted, types = redact_pii(text)\n        \n        assert 'CUST-12345' in redacted\n        assert 'ORD-67890' in redacted\n        assert len(types) == 0\n\n\nclass TestInsightSafety:\n    \n    @pytest.mark.skip(reason=\"Not implemented\")\n    def test_flags_medical_advice(self):\n        \"\"\"Insights with medical advice should be flagged.\"\"\"\n        insight = {\n            'summary': 'Customer mentioned health issues affecting their team'\n        }\n        concerns = validate_insight_safety(insight)\n        assert len(concerns) > 0\n    \n    @pytest.mark.skip(reason=\"Not implemented\")\n    def test_flags_low_confidence_high_stakes(self):\n        \"\"\"Low confidence + high stakes = flag for review.\"\"\"\n        insight = {\n            'type': 'churn_risk',\n            'confidence': 0.3,\n            'action': 'Cancel their contract immediately'\n        }\n        assert should_require_human_review(insight) == True",

      "tests/test_cost.py": "\"\"\"Tests for cost control.\"\"\"\n\nimport pytest\nfrom unittest.mock import Mock\n\n\nclass TestCostControl:\n    \n    def test_uses_cheaper_model_for_low_stakes(self):\n        \"\"\"Sentiment analysis should use GPT-3.5, not GPT-4.\"\"\"\n        # Sentiment is low stakes - shouldn't use expensive model\n        # Currently not implemented\n        pass\n    \n    def test_respects_token_budget(self):\n        \"\"\"Should not exceed token budget per customer.\"\"\"\n        pass\n    \n    def test_caching_reduces_costs(self):\n        \"\"\"Cached responses should not incur LLM costs.\"\"\"\n        pass",

      "api/__init__.py": "",

      "api/routes.py": "\"\"\"API routes for insights.\"\"\"\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\napp = FastAPI(title=\"Customer Intelligence API\")\n\n\nclass InsightResponse(BaseModel):\n    insight_type: str\n    summary: str\n    confidence: float\n    action: Optional[str]\n\n\n@app.get(\"/customers/{customer_id}/insights\")\nasync def get_insights(customer_id: str) -> List[InsightResponse]:\n    \"\"\"Get insights for a customer.\"\"\"\n    # TODO: Add authentication\n    # TODO: Add rate limiting\n    # TODO: Add caching\n    # TODO: Add audit logging\n    pass\n\n\n@app.post(\"/customers/{customer_id}/insights/refresh\")\nasync def refresh_insights(customer_id: str) -> List[InsightResponse]:\n    \"\"\"Force regenerate insights for a customer.\"\"\"\n    # TODO: Add authentication\n    # TODO: Add cost controls\n    pass",

      "requirements.txt": "openai>=1.0.0\nfastapi>=0.100.0\nuvicorn>=0.23.0\npydantic>=2.0.0\npsycopg2-binary>=2.9.0\nredis>=4.5.0\npytest>=7.0.0\npytest-asyncio>=0.21.0"
    }
  },

  "evaluation_rubric": {
    "risk_assessment": {
      "weight": 0.25,
      "criteria": {
        "excellent": "Systematically reviews RISKS.md and code. Identifies the four categories of risk: (1) Safety - PII exposure, bad advice, (2) Reliability - no fallbacks, no error handling, (3) Cost - no caching, wrong model selection, (4) Trust - no human review, no audit trail. Prioritizes safety and trust over performance.",
        "good": "Identifies most major risks. Some prioritization logic.",
        "poor": "Misses critical risks (PII) or focuses on wrong things (performance before safety)."
      }
    },
    "safety_thinking": {
      "weight": 0.25,
      "criteria": {
        "excellent": "Treats PII and bad advice as showstoppers. Implements: PII redaction before LLM calls, human review for high-stakes insights, confidence thresholds, audit trail. Proposes guardrails that Legal/Sales would accept.",
        "good": "Addresses main safety concerns. May miss edge cases.",
        "poor": "Treats safety as nice-to-have. Ships without PII redaction or review workflow."
      }
    },
    "production_readiness": {
      "weight": 0.20,
      "criteria": {
        "excellent": "Implements: error handling with fallbacks, caching strategy, model tiering (GPT-4 for high-stakes, 3.5 for low), timeout handling, cost tracking. Code would survive a production incident.",
        "good": "Addresses main reliability concerns. Some gaps.",
        "poor": "Prototype-quality code. Would fail in production."
      }
    },
    "stakeholder_communication": {
      "weight": 0.15,
      "criteria": {
        "excellent": "Produces a launch plan that addresses Legal (PII, liability), Sales (trust, override capability), Infra (cost controls), QA (testing approach). Clear about what's fixed, what's mitigated, what's accepted risk.",
        "good": "Some stakeholder awareness. Reasonable communication.",
        "poor": "Pure technical focus. Couldn't present to non-engineers."
      }
    },
    "pragmatic_judgment": {
      "weight": 0.15,
      "criteria": {
        "excellent": "Ships something. Makes smart scope cuts. Proposes phased launch (internal first, then beta, then GA). Acknowledges tradeoffs explicitly. Knows what NOT to fix now.",
        "good": "Balances shipping with safety reasonably.",
        "poor": "Either ships something dangerous or gets stuck trying to fix everything."
      }
    }
  },

  "expected_candidate_journey": {
    "first_10_minutes": [
      "Read README and RISKS.md thoroughly",
      "Understand the four stakeholder concerns (Legal, Sales, Infra, QA)",
      "Review analyzer.py to understand the core flow",
      "Identify critical issues: PII exposure, no error handling, no caching",
      "Prioritize: PII redaction > human review > error handling > cost optimization"
    ],
    "minutes_10_to_20": [
      "Enable PII redaction in the flow (use existing safety.py functions)",
      "Add try/catch around LLM calls with fallback behavior",
      "Implement human review flag for high-confidence churn predictions",
      "Fix config to enable safety features by default"
    ],
    "minutes_20_to_28": [
      "Add basic caching or model tiering to address cost",
      "Ensure tests pass",
      "Draft launch plan with phased rollout"
    ],
    "final_2_minutes": [
      "Document what's fixed and what remains",
      "Propose launch criteria: 'We can launch to internal users when X, beta when Y, GA when Z'"
    ]
  },

  "interviewer_signals": {
    "strong_positive": [
      "Reads RISKS.md before touching code",
      "Asks 'what's the worst thing that could happen?'",
      "Prioritizes PII redaction as a blocker",
      "Proposes human-in-the-loop for high-stakes insights",
      "Thinks about phased rollout, not big-bang launch",
      "Communicates tradeoffs explicitly",
      "Tests safety features, not just happy path"
    ],
    "red_flags": [
      "Jumps to cost optimization before safety",
      "Ignores PII concerns ('OpenAI is trustworthy')",
      "No error handling or fallbacks",
      "Ships without any human review mechanism",
      "Can't explain risks to non-technical stakeholders",
      "Focuses on making tests pass rather than making system safe",
      "No concept of phased launch or risk mitigation"
    ]
  }
}
