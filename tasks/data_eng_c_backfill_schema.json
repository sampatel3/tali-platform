{
  "task_id": "data_eng_c_backfill_schema",
  "name": "Historical Backfill and Schema Evolution",
  "role": "data_engineer",
  "duration_minutes": 30,
  "scenario": "Two requests landed on your desk this week:\n\n1. **Compliance audit (due in 3 days):** Legal needs a complete transaction history from Jan 2023 to present. The current pipeline only has data from July 2023 when it was deployed. You need to backfill 6 months of historical data.\n\n2. **Schema changes keep breaking things:** The source API added new fields twice this quarter. Each time, the pipeline failed and someone had to manually add columns to the warehouse. Product says more fields are coming. Make it handle schema changes automatically.\n\nThe pipeline runs daily and is currently working (for incremental loads). Don't break what's working while adding these capabilities.",
  "repo_structure": {
    "name": "transaction-pipeline",
    "files": {
      "README.md": "# Transaction Pipeline\n\nDaily incremental sync from Payment API to warehouse.\n\n## Current State\n- ✅ Daily incremental loads (working)\n- ❌ Historical backfill (not supported)\n- ❌ Schema evolution (manual intervention required)\n\n## Running\n```\npython -m pipeline.main --mode incremental\n```\n\n## Recent Incidents\n- 2024-01-10: Pipeline failed when API added 'risk_score' field\n- 2023-11-15: Pipeline failed when API added 'metadata' field\n- Both required manual ALTER TABLE and pipeline restart",
      "incidents/2024-01-10_schema_failure.md": "# Incident: Schema Mismatch Failure\n\n**Date:** 2024-01-10 02:15 UTC\n**Duration:** 4 hours\n**Impact:** No data loaded for Jan 9\n\n## What happened\nPayment API started returning a new field `risk_score` (float). Pipeline failed with:\n```\nKeyError: 'risk_score' - column does not exist in target table\n```\n\n## Resolution\n1. Manually ran: ALTER TABLE transactions ADD COLUMN risk_score FLOAT\n2. Restarted pipeline\n3. Re-ran for Jan 9 data\n\n## Prevention\nNeed automatic schema evolution. Pipeline should:\n1. Detect new columns in source\n2. Add them to target automatically\n3. Log changes for audit\n\n## Related\nSame thing happened on 2023-11-15 when 'metadata' field was added.",
      "incidents/backfill_request.md": "# Request: Historical Data Backfill\n\n**From:** Legal/Compliance\n**Priority:** High\n**Deadline:** 2024-01-18\n\n## Request\nFor the upcoming audit, we need complete transaction history from January 1, 2023 to present.\n\n## Current State\n- Pipeline deployed: July 1, 2023\n- Data available: July 1, 2023 - present\n- Data needed: January 1, 2023 - June 30, 2023 (6 months)\n\n## Requirements\n1. Backfill must be complete - every transaction\n2. Must not affect ongoing daily loads\n3. Must be idempotent (safe to re-run if it fails partway)\n4. Need audit log of what was backfilled\n\n## Source API\nThe Payment API supports historical queries:\n```\nGET /transactions?start_date=2023-01-01&end_date=2023-01-31\n```\n\nRate limit: 100 requests/minute, max 10,000 records per request.",
      "pipeline/__init__.py": "",
      "pipeline/main.py": "import argparse\nimport logging\nfrom pipeline.incremental import IncrementalLoader\nfrom pipeline.config import PipelineConfig\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--mode', choices=['incremental', 'backfill'], default='incremental')\n    parser.add_argument('--start-date', help='For backfill: start date (YYYY-MM-DD)')\n    parser.add_argument('--end-date', help='For backfill: end date (YYYY-MM-DD)')\n    args = parser.parse_args()\n    \n    config = PipelineConfig.load()\n    \n    if args.mode == 'incremental':\n        loader = IncrementalLoader(config)\n        result = loader.run()\n        logger.info(f\"Incremental load complete: {result}\")\n    else:\n        # TODO: Implement backfill mode\n        raise NotImplementedError(\"Backfill mode not yet implemented\")\n\n\nif __name__ == '__main__':\n    main()",
      "pipeline/config.py": "from dataclasses import dataclass\nfrom typing import Dict, Any\n\n\n@dataclass\nclass PipelineConfig:\n    api_base_url: str\n    api_key: str\n    target_table: str\n    checkpoint_table: str\n    schema_log_table: str\n    batch_size: int = 5000\n    \n    @classmethod\n    def load(cls) -> 'PipelineConfig':\n        return cls(\n            api_base_url='https://api.payments.example.com',\n            api_key='pk_live_xxx',\n            target_table='warehouse.transactions',\n            checkpoint_table='warehouse.pipeline_checkpoints',\n            schema_log_table='warehouse.schema_changes',\n            batch_size=5000\n        )",
      "pipeline/incremental.py": "import logging\nfrom datetime import datetime, date, timedelta\nfrom typing import Dict, Any, List\nfrom dataclasses import dataclass\nfrom pipeline.config import PipelineConfig\nfrom pipeline.source import PaymentAPI\nfrom pipeline.target import WarehouseDB\nfrom pipeline.checkpoint import CheckpointManager\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass LoadResult:\n    records_loaded: int\n    start_date: date\n    end_date: date\n    duration_seconds: float\n\n\nclass IncrementalLoader:\n    \"\"\"Daily incremental loader. DO NOT BREAK THIS - it's working in production.\"\"\"\n    \n    def __init__(self, config: PipelineConfig):\n        self.config = config\n        self.api = PaymentAPI(config.api_base_url, config.api_key)\n        self.db = WarehouseDB(config.target_table)\n        self.checkpoint = CheckpointManager(config.checkpoint_table)\n    \n    def run(self) -> LoadResult:\n        start_time = datetime.now()\n        \n        # Get last checkpoint\n        last_date = self.checkpoint.get_last_date('transactions')\n        if last_date:\n            start_date = last_date + timedelta(days=1)\n        else:\n            start_date = date.today() - timedelta(days=7)\n        \n        end_date = date.today() - timedelta(days=1)  # Yesterday\n        \n        if start_date > end_date:\n            logger.info(\"No new data to load\")\n            return LoadResult(0, start_date, end_date, 0)\n        \n        logger.info(f\"Loading data from {start_date} to {end_date}\")\n        \n        total_records = 0\n        current_date = start_date\n        \n        while current_date <= end_date:\n            records = self._load_date(current_date)\n            total_records += records\n            current_date += timedelta(days=1)\n        \n        # Update checkpoint\n        self.checkpoint.save('transactions', end_date)\n        \n        duration = (datetime.now() - start_time).total_seconds()\n        return LoadResult(total_records, start_date, end_date, duration)\n    \n    def _load_date(self, target_date: date) -> int:\n        \"\"\"Load all data for a single date.\"\"\"\n        logger.info(f\"Loading {target_date}\")\n        \n        offset = 0\n        total = 0\n        \n        while True:\n            batch = self.api.fetch_transactions(\n                start_date=target_date,\n                end_date=target_date,\n                offset=offset,\n                limit=self.config.batch_size\n            )\n            \n            if not batch:\n                break\n            \n            # BUG: If source has new columns, this fails\n            # Currently requires manual ALTER TABLE\n            self.db.insert_batch(batch)\n            \n            total += len(batch)\n            offset += len(batch)\n            \n            if len(batch) < self.config.batch_size:\n                break\n        \n        logger.info(f\"Loaded {total} records for {target_date}\")\n        return total",
      "pipeline/source.py": "from datetime import date\nfrom typing import List, Dict, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass PaymentAPI:\n    \"\"\"Client for the Payment API.\"\"\"\n    \n    def __init__(self, base_url: str, api_key: str):\n        self.base_url = base_url\n        self.api_key = api_key\n    \n    def fetch_transactions(\n        self,\n        start_date: date,\n        end_date: date,\n        offset: int = 0,\n        limit: int = 1000\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Fetch transactions from API.\n        \n        Returns list of transaction records.\n        Schema may change over time (new fields added).\n        \"\"\"\n        # Mock implementation\n        # In production, makes HTTP request to:\n        # GET {base_url}/transactions?start_date={start_date}&end_date={end_date}&offset={offset}&limit={limit}\n        \n        return []  # Mock\n    \n    def get_schema(self) -> Dict[str, str]:\n        \"\"\"\n        Get current schema from API.\n        \n        Returns dict of field_name -> field_type.\n        This can be used to detect schema changes.\n        \"\"\"\n        # GET {base_url}/schema/transactions\n        return {\n            'transaction_id': 'string',\n            'amount': 'decimal',\n            'currency': 'string',\n            'status': 'string',\n            'created_at': 'timestamp',\n            'customer_id': 'string',\n            'metadata': 'json',  # Added 2023-11-15\n            'risk_score': 'float',  # Added 2024-01-10\n            # More fields may be added in the future\n        }",
      "pipeline/target.py": "from typing import List, Dict, Any, Set\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass WarehouseDB:\n    \"\"\"Connection to the data warehouse.\"\"\"\n    \n    def __init__(self, table: str):\n        self.table = table\n    \n    def get_schema(self) -> Dict[str, str]:\n        \"\"\"Get current table schema.\"\"\"\n        # SELECT column_name, data_type FROM information_schema.columns\n        # WHERE table_name = '{table}'\n        return {\n            'transaction_id': 'varchar',\n            'amount': 'decimal',\n            'currency': 'varchar',\n            'status': 'varchar',\n            'created_at': 'timestamp',\n            'customer_id': 'varchar',\n            'metadata': 'jsonb',\n            # Note: risk_score is missing - was added manually after incident\n        }\n    \n    def add_column(self, column_name: str, column_type: str):\n        \"\"\"Add a new column to the table.\"\"\"\n        # ALTER TABLE {table} ADD COLUMN {column_name} {column_type}\n        logger.info(f\"Adding column {column_name} ({column_type}) to {self.table}\")\n    \n    def insert_batch(self, records: List[Dict[str, Any]]):\n        \"\"\"\n        Insert batch of records.\n        \n        Raises KeyError if record has columns not in table.\n        \"\"\"\n        # INSERT INTO {table} (...) VALUES (...)\n        logger.debug(f\"Inserting {len(records)} records\")\n    \n    def upsert_batch(self, records: List[Dict[str, Any]], key_columns: List[str]):\n        \"\"\"Insert or update based on key columns.\"\"\"\n        # INSERT ... ON CONFLICT (key_columns) DO UPDATE\n        logger.debug(f\"Upserting {len(records)} records\")\n    \n    def delete_partition(self, partition_column: str, partition_value: Any):\n        \"\"\"Delete all records in a partition.\"\"\"\n        # DELETE FROM {table} WHERE {partition_column} = {partition_value}\n        logger.debug(f\"Deleting partition {partition_column}={partition_value}\")\n    \n    def get_partitions(self, partition_column: str) -> Set[Any]:\n        \"\"\"Get all existing partition values.\"\"\"\n        # SELECT DISTINCT {partition_column} FROM {table}\n        return set()",
      "pipeline/checkpoint.py": "from datetime import date\nfrom typing import Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass CheckpointManager:\n    \"\"\"Manages pipeline checkpoints.\"\"\"\n    \n    def __init__(self, table: str):\n        self.table = table\n        self._checkpoints = {}  # Mock storage\n    \n    def get_last_date(self, pipeline_name: str) -> Optional[date]:\n        \"\"\"Get the last successfully processed date.\"\"\"\n        return self._checkpoints.get(pipeline_name)\n    \n    def save(self, pipeline_name: str, processed_date: date):\n        \"\"\"Save checkpoint after successful processing.\"\"\"\n        self._checkpoints[pipeline_name] = processed_date\n        logger.info(f\"Checkpoint saved: {pipeline_name} -> {processed_date}\")",
      "pipeline/backfill.py": "# TODO: Implement backfill capability\n#\n# Requirements:\n# 1. Load historical data for a date range\n# 2. Must be idempotent (safe to re-run)\n# 3. Must not interfere with daily incremental loads\n# 4. Should support parallel loading for performance\n# 5. Must log what was backfilled for audit\n#\n# Considerations:\n# - API rate limit: 100 requests/minute\n# - How to handle partial failures?\n# - Should we use separate checkpoint for backfill?",
      "pipeline/schema_evolution.py": "# TODO: Implement automatic schema evolution\n#\n# Requirements:\n# 1. Detect new columns in source API\n# 2. Add them to target table automatically\n# 3. Log all schema changes\n# 4. Use appropriate default values\n#\n# Considerations:\n# - What about column type changes? (probably don't auto-handle)\n# - What about column removals? (probably ignore)\n# - Should this run before every load or on failure?",
      "tests/__init__.py": "",
      "tests/test_backfill.py": "import pytest\nfrom datetime import date\n\n\nclass TestBackfill:\n    def test_backfill_date_range(self):\n        \"\"\"Should load all data for specified date range\"\"\"\n        pass\n    \n    def test_backfill_is_idempotent(self):\n        \"\"\"Running backfill twice should produce same result\"\"\"\n        pass\n    \n    def test_backfill_does_not_affect_incremental_checkpoint(self):\n        \"\"\"Backfill should not move the incremental checkpoint\"\"\"\n        pass\n    \n    def test_backfill_handles_partial_failure(self):\n        \"\"\"Should be resumable after failure\"\"\"\n        pass\n    \n    def test_backfill_respects_rate_limit(self):\n        \"\"\"Should not exceed API rate limits\"\"\"\n        pass\n\n\nclass TestSchemaEvolution:\n    def test_detects_new_columns(self):\n        \"\"\"Should detect columns in source but not in target\"\"\"\n        pass\n    \n    def test_adds_new_columns(self):\n        \"\"\"Should add detected columns to target\"\"\"\n        pass\n    \n    def test_logs_schema_changes(self):\n        \"\"\"Should log all schema changes for audit\"\"\"\n        pass\n    \n    def test_does_not_remove_columns(self):\n        \"\"\"Should not remove columns that exist in target but not source\"\"\"\n        pass\n    \n    def test_handles_type_mapping(self):\n        \"\"\"Should map API types to warehouse types correctly\"\"\"\n        # API 'string' -> Warehouse 'varchar'\n        # API 'decimal' -> Warehouse 'decimal'\n        # API 'json' -> Warehouse 'jsonb'\n        pass",
      "requirements.txt": "requests>=2.28.0\npsycopg2-binary>=2.9.0\npytest>=7.0.0"
    }
  },
  "evaluation_rubric": {
    "understanding": {
      "weight": 0.15,
      "criteria": {
        "excellent": "Clearly understands both requirements. Notes the compliance deadline pressure. Reads incident reports to understand schema failure pattern. Identifies that incremental loader is working and must not be broken.",
        "good": "Understands main requirements. Some context awareness.",
        "poor": "Misunderstands requirements or misses constraints."
      }
    },
    "backfill_design": {
      "weight": 0.25,
      "criteria": {
        "excellent": "Designs idempotent backfill: delete partition before loading OR use upsert. Separate checkpoint to not affect incremental. Considers parallelization (by date). Handles rate limiting (100 req/min). Plans audit logging.",
        "good": "Working backfill design. Idempotent. Some consideration of constraints.",
        "poor": "Non-idempotent backfill or breaks incremental loads."
      }
    },
    "schema_evolution_design": {
      "weight": 0.25,
      "criteria": {
        "excellent": "Compares source schema vs target schema. Adds missing columns before load (not on failure). Maps types correctly (string->varchar, json->jsonb). Logs to schema_log_table. Considers: runs every load? Or detect and prompt?",
        "good": "Working schema evolution. Adds columns automatically.",
        "poor": "Incomplete or error-prone implementation."
      }
    },
    "implementation": {
      "weight": 0.2,
      "criteria": {
        "excellent": "Clean implementation of both features. Integrates with existing code without modifying working incremental loader. Handles edge cases. Good error handling.",
        "good": "Working implementations. Some edge cases handled.",
        "poor": "Implementations don't work or break existing code."
      }
    },
    "testing_validation": {
      "weight": 0.15,
      "criteria": {
        "excellent": "Tests backfill idempotency. Tests schema detection. Verifies incremental still works. May do manual testing with sample data.",
        "good": "Some testing. Main scenarios covered.",
        "poor": "No testing."
      }
    }
  },
  "expected_approaches": {
    "backfill": [
      "Use delete-then-insert per date partition for idempotency",
      "Separate backfill checkpoint (or no checkpoint, rely on idempotency)",
      "Process dates in parallel with rate limit awareness",
      "Log each completed date for audit trail"
    ],
    "schema_evolution": [
      "Before each load: compare source.get_schema() vs target.get_schema()",
      "For new columns: call target.add_column() with appropriate type",
      "Log to schema_log_table: column_name, type, added_at, added_by",
      "Type mapping: API types to warehouse types"
    ]
  }
}
