{
  "task_id": "data_eng_b_cdc_fix",
  "name": "Fix the Broken Data Sync",
  "role": "data_engineer",
  "duration_minutes": 15,
  "claude_budget_limit_usd": 5,
  "scenario": "The finance team reports that the analytics dashboard shows different numbers than the source system. They say 'the sync is broken' and need it fixed ASAP because month-end reporting is tomorrow.\n\nSomeone on the team built a CDC (Change Data Capture) sync 6 months ago. It mostly works but clearly something's wrong. The repo has the sync code, some comparison queries showing the discrepancies, and logs from recent runs.\n\nFind the bugs and fix them.",
  "repo_structure": {
    "name": "cdc-sync",
    "files": {
      "README.md": "# CDC Sync Service\n\nSyncs data from source Postgres to analytics warehouse.\n\n## How it works\n1. Fetches records modified since last sync (using updated_at)\n2. Compares with existing warehouse data\n3. Applies inserts, updates, deletes\n\n## Running\n```\npython -m sync.main --source orders --target analytics.orders\n```\n\n## Current Status\nWARNING: Finance reporting discrepancies - investigation needed",
      "discrepancies/orders_comparison.sql": "-- Run on 2024-01-15 to investigate discrepancies\n-- Source (Postgres) vs Target (Warehouse)\n\n-- Total row count\n-- Source: 45,832\n-- Target: 45,891  -- 59 extra rows in target?!\n\n-- Total revenue\n-- Source: $2,847,293.47\n-- Target: $2,851,102.33  -- $3,808.86 difference\n\n-- Specific discrepancies found:\n\n-- 1. Deleted orders still in target\nSELECT order_id FROM analytics.orders\nWHERE order_id NOT IN (SELECT order_id FROM source.orders);\n-- Returns: 47 order_ids (these were deleted in source)\n\n-- 2. Duplicate orders in target\nSELECT order_id, COUNT(*) FROM analytics.orders\nGROUP BY order_id HAVING COUNT(*) > 1;\n-- Returns: 12 order_ids with duplicates\n\n-- 3. Amount mismatches\nSELECT s.order_id, s.amount as source_amount, t.amount as target_amount\nFROM source.orders s\nJOIN analytics.orders t ON s.order_id = t.order_id\nWHERE s.amount != t.amount;\n-- Returns: 23 orders with different amounts (updates not applied?)",
      "logs/sync_2024-01-14.log": "2024-01-14 01:00:00 INFO Starting sync for orders\n2024-01-14 01:00:01 INFO Last sync: 2024-01-13 01:00:00\n2024-01-14 01:00:02 INFO Fetching changes since 2024-01-13 01:00:00\n2024-01-14 01:00:05 INFO Found 234 changed records\n2024-01-14 01:00:05 INFO Processing inserts: 189\n2024-01-14 01:00:06 INFO Processing updates: 45\n2024-01-14 01:00:06 INFO Processing deletes: 0\n2024-01-14 01:00:07 INFO Sync complete. Duration: 7s\n\n2024-01-14 01:00:07 WARNING No deletes detected - is this expected?",
      "logs/sync_2024-01-15.log": "2024-01-15 01:00:00 INFO Starting sync for orders\n2024-01-15 01:00:01 INFO Last sync: 2024-01-14 01:00:00\n2024-01-15 01:00:02 INFO Fetching changes since 2024-01-14 01:00:00\n2024-01-15 01:00:03 INFO Found 156 changed records\n2024-01-15 01:00:03 INFO Processing inserts: 145\n2024-01-15 01:00:04 INFO Processing updates: 11\n2024-01-15 01:00:04 INFO Processing deletes: 0\n2024-01-15 01:00:05 INFO Sync complete. Duration: 5s\n\n2024-01-15 01:00:05 WARNING No deletes detected - is this expected?\n2024-01-15 01:00:05 DEBUG Sample insert: order_id=ORD-98234, amount=149.99\n2024-01-15 01:00:05 DEBUG Sample insert: order_id=ORD-98234, amount=149.99\n2024-01-15 01:00:05 DEBUG Duplicate? Same order_id in batch",
      "sync/__init__.py": "",
      "sync/main.py": "import argparse\nimport logging\nfrom sync.cdc import CDCSync\nfrom sync.config import SyncConfig\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--source', required=True)\n    parser.add_argument('--target', required=True)\n    args = parser.parse_args()\n\n    config = SyncConfig.load(args.source)\n    sync = CDCSync(config)\n\n    result = sync.run()\n    logger.info(f\"Sync complete: {result}\")\n\n\nif __name__ == '__main__':\n    main()",
      "sync/config.py": "from dataclasses import dataclass\nfrom typing import List\n\n\n@dataclass\nclass SyncConfig:\n    source_table: str\n    target_table: str\n    primary_key: List[str]\n    timestamp_column: str = 'updated_at'\n    batch_size: int = 1000\n\n    @classmethod\n    def load(cls, source_name: str) -> 'SyncConfig':\n        # In production, loads from config file\n        configs = {\n            'orders': SyncConfig(\n                source_table='public.orders',\n                target_table='analytics.orders',\n                primary_key=['order_id'],\n                timestamp_column='updated_at'\n            )\n        }\n        return configs[source_name]",
      "sync/cdc.py": "import logging\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom sync.config import SyncConfig\nfrom sync.database import SourceDB, TargetDB\nfrom sync.checkpoint import CheckpointStore\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass SyncResult:\n    inserts: int\n    updates: int\n    deletes: int\n    duration_seconds: float\n\n\nclass CDCSync:\n    def __init__(self, config: SyncConfig):\n        self.config = config\n        self.source = SourceDB(config.source_table)\n        self.target = TargetDB(config.target_table)\n        self.checkpoint = CheckpointStore()\n\n    def run(self) -> SyncResult:\n        start = datetime.now()\n\n        # Get last sync time\n        last_sync = self.checkpoint.get_last_sync(self.config.source_table)\n        logger.info(f\"Last sync: {last_sync}\")\n\n        # Fetch changed records from source\n        changed_records = self.source.fetch_changes(\n            since=last_sync,\n            timestamp_col=self.config.timestamp_column\n        )\n        logger.info(f\"Found {len(changed_records)} changed records\")\n\n        # Categorize changes\n        inserts, updates, deletes = self._categorize_changes(changed_records)\n\n        logger.info(f\"Processing inserts: {len(inserts)}\")\n        logger.info(f\"Processing updates: {len(updates)}\")\n        logger.info(f\"Processing deletes: {len(deletes)}\")\n\n        # Apply changes\n        self._apply_inserts(inserts)\n        self._apply_updates(updates)\n        self._apply_deletes(deletes)\n\n        # Update checkpoint\n        self.checkpoint.save(self.config.source_table, datetime.now())\n\n        duration = (datetime.now() - start).total_seconds()\n        return SyncResult(\n            inserts=len(inserts),\n            updates=len(updates),\n            deletes=len(deletes),\n            duration_seconds=duration\n        )\n\n    def _categorize_changes(self, records: List[Dict]) -> Tuple[List, List, List]:\n        \"\"\"Categorize records into inserts, updates, deletes.\"\"\"\n        inserts = []\n        updates = []\n        deletes = []  # BUG: This is never populated!\n\n        # Get existing keys from target\n        existing_keys = self.target.get_existing_keys(self.config.primary_key)\n\n        for record in records:\n            key = self._extract_key(record)\n\n            if key in existing_keys:\n                updates.append(record)\n            else:\n                inserts.append(record)\n\n        # BUG: We never check for deletes!\n        # Records that exist in target but not in source should be deleted\n        # But we only look at changed_records from source, not what's missing\n\n        return inserts, updates, deletes\n\n    def _extract_key(self, record: Dict) -> tuple:\n        return tuple(record[k] for k in self.config.primary_key)\n\n    def _apply_inserts(self, records: List[Dict]):\n        # BUG: No deduplication - if same record appears twice in batch,\n        # it gets inserted twice\n        for record in records:\n            self.target.insert(record)\n\n    def _apply_updates(self, records: List[Dict]):\n        for record in records:\n            key = self._extract_key(record)\n            self.target.update(key, record)\n\n    def _apply_deletes(self, records: List[Dict]):\n        for record in records:\n            key = self._extract_key(record)\n            self.target.delete(key)",
      "sync/database.py": "from typing import List, Dict, Set, Any\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass SourceDB:\n    \"\"\"Mock source database connection.\"\"\"\n\n    def __init__(self, table: str):\n        self.table = table\n\n    def fetch_changes(self, since: datetime, timestamp_col: str) -> List[Dict]:\n        \"\"\"Fetch records modified since given timestamp.\"\"\"\n        # In production, executes:\n        # SELECT * FROM {table} WHERE {timestamp_col} > {since}\n        #\n        # Note: This only gets records that EXIST and were modified.\n        # It cannot detect hard deletes (rows that no longer exist).\n        return []  # Mock\n\n    def fetch_all_keys(self, key_columns: List[str]) -> Set[tuple]:\n        \"\"\"Fetch all primary keys from source.\"\"\"\n        # SELECT {key_columns} FROM {table}\n        return set()  # Mock\n\n\nclass TargetDB:\n    \"\"\"Mock target database connection.\"\"\"\n\n    def __init__(self, table: str):\n        self.table = table\n\n    def get_existing_keys(self, key_columns: List[str]) -> Set[tuple]:\n        \"\"\"Get all existing keys in target.\"\"\"\n        return set()  # Mock\n\n    def insert(self, record: Dict):\n        \"\"\"Insert a record.\"\"\"\n        logger.debug(f\"INSERT: {record}\")\n\n    def update(self, key: tuple, record: Dict):\n        \"\"\"Update a record by key.\"\"\"\n        logger.debug(f\"UPDATE {key}: {record}\")\n\n    def delete(self, key: tuple):\n        \"\"\"Delete a record by key.\"\"\"\n        logger.debug(f\"DELETE: {key}\")",
      "sync/checkpoint.py": "from datetime import datetime\nfrom typing import Optional\n\n\nclass CheckpointStore:\n    \"\"\"Tracks last sync time per table.\"\"\"\n\n    def __init__(self):\n        self._checkpoints = {}\n\n    def get_last_sync(self, table: str) -> Optional[datetime]:\n        return self._checkpoints.get(table)\n\n    def save(self, table: str, timestamp: datetime):\n        self._checkpoints[table] = timestamp",
      "tests/__init__.py": "",
      "tests/test_cdc.py": "import pytest\nfrom sync.cdc import CDCSync\nfrom sync.config import SyncConfig\n\n\nclass TestCDCSync:\n    @pytest.fixture\n    def config(self):\n        return SyncConfig(\n            source_table='test_source',\n            target_table='test_target',\n            primary_key=['id']\n        )\n\n    def test_detects_inserts(self, config):\n        \"\"\"New records in source should be inserted\"\"\"\n        pass\n\n    def test_detects_updates(self, config):\n        \"\"\"Modified records should be updated\"\"\"\n        pass\n\n    def test_detects_deletes(self, config):\n        \"\"\"FAILING: Records deleted from source should be deleted from target\"\"\"\n        # sync = CDCSync(config)\n        #\n        # source_records = [{'id': 1}, {'id': 2}]  # id=3 was deleted\n        # target_records = [{'id': 1}, {'id': 2}, {'id': 3}]\n        #\n        # result = sync.run()\n        # assert result.deletes == 1\n        pass\n\n    def test_handles_duplicates_in_batch(self, config):\n        \"\"\"FAILING: Same record twice in batch should not create duplicate\"\"\"\n        # sync = CDCSync(config)\n        #\n        # # Same order appears twice (e.g., updated twice in source window)\n        # changed = [{'id': 1, 'amount': 100}, {'id': 1, 'amount': 100}]\n        #\n        # result = sync.run(changed)\n        # # Should be 1 insert, not 2\n        # assert result.inserts == 1\n        pass\n\n\nclass TestDeleteDetection:\n    def test_detects_hard_deletes(self):\n        \"\"\"Should detect records that were deleted from source\"\"\"\n        # The current approach (fetch changes since X) cannot detect deletes\n        # Need a different strategy:\n        # Option 1: Full key comparison (SELECT all keys from source, compare to target)\n        # Option 2: Soft deletes in source (is_deleted flag)\n        # Option 3: CDC log/WAL from source database\n        pass",
      "requirements.txt": "psycopg2-binary>=2.9.0\npytest>=7.0.0"
    }
  },
  "evaluation_rubric": {
    "diagnosis": {
      "weight": 0.3,
      "criteria": {
        "excellent": "Identifies all 3 bugs from logs + code: (1) Deletes never detected - fetch_changes can't see deleted rows, (2) Duplicates in batch - no dedup before insert, (3) Updates may be misclassified as inserts if key check is wrong. Connects bugs to discrepancy report (47 extra rows = undeleted, 12 duplicates, 23 amount mismatches).",
        "good": "Identifies main bugs (deletes, duplicates). May miss nuances.",
        "poor": "Can't identify bugs. Proposes unrelated fixes."
      }
    },
    "solution_approach": {
      "weight": 0.25,
      "criteria": {
        "excellent": "For deletes: Proposes full key comparison OR soft delete strategy. Discusses tradeoffs (full scan expensive but reliable). For duplicates: Dedupe by key before processing. Considers batch atomicity.",
        "good": "Reasonable solutions for main issues.",
        "poor": "Solutions don't address root cause."
      }
    },
    "implementation": {
      "weight": 0.25,
      "criteria": {
        "excellent": "Fixes _categorize_changes to detect deletes (compare all source keys vs target keys). Adds deduplication in _apply_inserts. Code is clean, handles edge cases.",
        "good": "Working fixes for main issues.",
        "poor": "Fixes incomplete or introduce new bugs."
      }
    },
    "urgency_handling": {
      "weight": 0.2,
      "criteria": {
        "excellent": "Recognizes month-end urgency. Prioritizes: (1) Fix delete detection to stop bleeding, (2) Fix duplicates to prevent future issues, (3) May suggest one-time cleanup script for existing bad data. Considers running a full reconciliation.",
        "good": "Gets fixes done. Some urgency awareness.",
        "poor": "No urgency awareness. Over-engineers."
      }
    }
  },
  "expected_bugs": [
    "Delete detection: fetch_changes() only returns modified records, not deleted ones. Need to compare all source keys vs target keys.",
    "Duplicate inserts: If same record appears twice in changed_records batch (updated twice in window), it gets inserted twice. Need to dedupe by primary key.",
    "The warning 'No deletes detected - is this expected?' appears in every log but was ignored."
  ],
  "expected_fixes": [
    "Add delete detection: fetch all keys from source, compare to target, delete missing ones",
    "Dedupe inserts: use dict by primary key to ensure each key only processed once",
    "Optionally: add a reconciliation/repair mode for the existing data"
  ]
}
