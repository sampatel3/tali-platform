# RALPH_TASK.md ‚Äî TAALI Platform Improvement Backlog

**Rewritten:** 2026-02-22
**Source:** Deep codebase review (`/Users/sampatel/tali-platform`) + live site crawl (`https://frontend-psi-navy-15.vercel.app/`) + scoring engine analysis
**Methodology:** Read all scoring source files (`scoring_core.py`, `service.py`, `rules.py`, `analytics.py`, `metadata.py`), both task JSON definitions, assessment runtime, frontend display components (`CandidateResultsTab`, `CandidateDetailPage`, `scoringDimensions.ts`), and extracted UI strings from the compiled JS bundle on the live site.

---

## Mission

TAALI's only job is to be **the most accurate, credible, and trustworthy measure of how a candidate collaborates with AI to do real engineering work.** Everything else ‚Äî integrations, billing, scheduling, credentials ‚Äî is plumbing that exists to support this mission. It is secondary.

The world of work is changing faster than any hiring tool has adapted to. Prompting and AI-agent use are becoming foundational to engineering output. TAALI is the first platform built for that reality. The question is: does the product currently live up to that promise? After a full codebase and live site review, the answer is: **partially ‚Äî the scaffolding is excellent, but the signal quality and UX trust layer are incomplete.**

This document is a strict prioritisation of what to build next, split into two categories only:

1. **ACTIVE BACKLOG** ‚Äî Core assessment quality and UI/UX polish. P0‚ÄìP2 only. These are the things that determine whether a first paying customer trusts and uses this product.
2. **FUTURE BACKLOG** ‚Äî Everything else. Valid, but not now.

---

## Priority & Effort Legend

| Priority | Meaning |
|----------|---------|
| **P0** | Broken, misleading, or will kill recruiter trust on first use. Must fix before any demo. |
| **P1** | Core IP or UX work. Determines whether the product is a 7/10 or a 9/10. Build next. |
| **P2** | Valuable but not urgent. Compounds the product over time. |

| Effort | Meaning |
|--------|---------|
| **S** | < 1 day |
| **M** | 1‚Äì3 days |
| **L** | 3‚Äì7 days |
| **XL** | > 1 week |

---

## ACTIVE BACKLOG ‚Äî Assessment Quality

These tasks affect the accuracy, depth, and defensibility of TAALI's core scoring signal.

---

### SCORE-001 ‚Äî Verify per-prompt metadata capture from CLI transcript

**Priority:** P0 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
`scoring_core.py` (lines 234‚Äì383) scores several critical metrics by reading per-prompt boolean flags: `code_snippet_included`, `error_message_included`, `line_number_referenced`, `file_reference`, `references_previous`, `retry_after_failure`, `paste_detected`, `code_before`, `code_after`, `code_diff_lines_added`, `code_diff_lines_removed`.

These flags determine scores for `context_provision` (15% weight), `independence_efficiency` (20%), and `response_utilization` (10%) ‚Äî totalling **45% of the final score**.

However, the assessment terminal is a raw xterm.js instance that streams bytes from a WebSocket (`AssessmentTerminal.jsx`). The Claude CLI runs inside an E2B sandbox. There is no explicit code found in the reviewed files that extracts `code_before`/`code_after` snapshots at each prompt boundary, or that sets the boolean flags like `code_snippet_included` from parsing the prompt text.

**Risk:** If these fields are being stored as `null` or `0` for every prompt, then 45% of the final score is always defaulting to the worst-case heuristics. A candidate who includes perfect code snippets and error messages would score the same as one who writes "fix it please."

**What to do:**
1. Add a logging endpoint or test harness that prints the raw `ai_prompts` JSON for a completed assessment. Confirm which fields are populated.
2. If `code_snippet_included`, `error_message_included`, `line_number_referenced` are not being extracted, implement a `_extract_prompt_metadata(prompt_text: str) -> dict` function in `scoring_core.py` that detects:
   - Code snippets: presence of triple-backtick fences or 4-space-indented blocks
   - Error messages: keywords like `Traceback`, `Error:`, `Exception:`, `FAILED`, `assert`
   - Line/file references: patterns like `line 42`, `file.py`, `src/`, `:123:`
   - Prior attempt mentions: existing `ATTEMPT_PATTERNS` in `scoring_core.py` (these are fine)
3. Apply this extraction at scoring time in `service.py::calculate_mvp_score()` before calling category scorers.

**Acceptance criteria:** A test case that passes a prompt containing `"I tried calling the function but got: Traceback (most recent call last): ...File 'app.py', line 42"` and confirms `error_message_included=True` and `line_number_referenced=True` in scoring input.

---

### SCORE-002 ‚Äî Expand VAGUE_PATTERNS to cover real vague prompts

**Priority:** P0 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
`rules.py` (lines 18‚Äì22) has only 3 VAGUE_PATTERNS:
```python
VAGUE_PATTERNS = [
    r"^(help|fix|broken|not working|doesn't work|error|issue)\.?$",
    r"^(what's wrong|why isn't this working)\??$",
    r"^(please help|can you help|help me)\.?$",
]
```

These only match the most extreme single-word or single-phrase prompts. A candidate who writes "make it work", "write the tests for me", "do the whole thing", "just finish this", "rewrite everything", "implement the solution" will score 10/10 on `vagueness_score`. These are actually the most common vague behaviors in real assessments.

**What to do:**
Add the following patterns to `VAGUE_PATTERNS` in `rules.py`:
```python
r"^(make|do|fix|build|write|implement|create|complete|do|finish)\s+(it|this|that|the|everything|all|the whole|the complete)\b",
r"^(just|simply|please)\s+(make|do|fix|build|write|implement|finish)\b",
r"^(write|create|implement|build)\s+(the\s+)?(whole|entire|complete|full)\b",
r"^(make it work|get it working|make this work|make this pass|get the tests passing)\b",
r"^(rewrite|redo|redo everything|start over)\b",
```

**Acceptance criteria:** Unit test in `test_scoring.py` that verifies "make it work", "write the whole solution", "just implement this" all score 0 on `vagueness_score`.

---

### SCORE-003 ‚Äî Normalize weights when cv_match is None

**Priority:** P0 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
In `service.py` (lines 101‚Äì110), the weighted final score divides by `total_weight`. The cv_match category has `score = None` when no CV is uploaded or CV matching is disabled. Looking at the code:

```python
for cat_key, w in used_weights.items():
    score = category_scores.get(cat_key)
    if score is not None:
        weighted_sum += score * w * 10.0
        total_weight += w
```

This correctly excludes `None` scores from the denominator. **However**, in `scoring_core.py` `_score_cv_match()` returns `score: None` but `_score_task_completion()` etc. always return a numeric score. Verify that the cv_match `None` path is exercised in tests and that the weight redistribution is numerically correct (i.e. the sum of remaining weights divides properly).

Additionally, the CATEGORY_WEIGHTS in `scoring_core.py` assign `cv_match: 0.05`. When this is excluded, the other categories effectively each get a ~5.3% boost. This is correct behaviour but should be documented and tested explicitly.

**What to do:**
1. Add a test: assessment with `cv_match_result=None` should produce `final_score == calculate_mvp_score_without_cv / 0.95` (within floating point tolerance).
2. Add a comment in `service.py` line ~108 explaining the weight redistribution.

**Acceptance criteria:** Test passes. Comment added. No silent score inflation for candidates whose CVs were never uploaded.

---

### SCORE-004 ‚Äî Fix scoring of "independence" first_prompt_delay for complex tasks

**Priority:** P1 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
`scoring_core.py` (lines 281‚Äì292) scores `first_prompt_delay`:
- ‚â• 240s (4 min): 10/10
- ‚â• 120s (2 min): 8/10
- ‚â• 60s (1 min): 5/10
- ‚â• 30s: 3/10
- < 30s: 1/10

This rewards waiting before prompting. The intent is that a strong candidate reads the problem first. But both tasks in the library (`ai_eng_super_production_launch.json` and `data_eng_super_platform_crisis.json`) are complex multi-file repos with README, RISKS.md, architecture docs, and multiple source files. Reading the problem statement AND exploring the codebase realistically takes 5‚Äì10 minutes before a candidate would make their first prompt.

The current thresholds mean that a candidate who spends 8 minutes reading and exploring gets the same score (10/10) as one who waits exactly 4 minutes. Meanwhile, a candidate who takes 3 minutes to read a long README and then makes a well-structured prompt gets 8/10 vs. their 10/10 peer who just waited longer.

More importantly: the `first_prompt_delay` field in `service.py` reads from `prompts[0].get("time_since_assessment_start_ms")`. If this field is not being set (see SCORE-001), all candidates score 1/10 on this metric.

**What to do:**
1. Resolve SCORE-001 first (ensure `time_since_assessment_start_ms` is populated).
2. Adjust thresholds to be task-aware: tasks should declare `min_reading_time_seconds` in their JSON (e.g., `ai_eng_super_production_launch` is a complex repo, suggest 300s). Use this in scoring instead of a hardcoded 240s threshold.
3. Until task-level config exists, raise the default threshold to 300s (5 min) for top score, 180s (3 min) for 8/10.

**Acceptance criteria:** Task JSON can optionally declare `"scoring_hints": {"min_reading_time_seconds": 300}`. The scoring engine reads this and adjusts `first_prompt_delay` thresholds accordingly.

---

### SCORE-005 ‚Äî Add prompt specificity evolution metric

**Priority:** P1 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
The current scoring evaluates prompt quality as a static aggregate ‚Äî average vagueness, average specificity across all prompts. But the trajectory matters significantly more than the average. A candidate who starts with vague prompts and gets progressively more specific (learning the problem, adding context, citing errors) is demonstrating strong AI-collaboration instincts. A candidate whose prompts degrade over time (increasingly short, increasingly frustrated) is a red flag.

There is a `prompt_quality_trend` in `service.py` (lines 175‚Äì187) but it only compares average word count between first and second halves. Word count is a poor proxy for quality.

**What to do:**
Add `_score_prompt_evolution(prompts: list) -> Dict[str, Any]` to `scoring_core.py`:

1. Split prompts into thirds (early, mid, late).
2. For each third, compute: specificity rate (non-vague prompts), context rate (prompts with code/error), average word count.
3. Score = improvement from early to late thirds. If context rate grows from <30% to >60%, that's a strong positive signal. If it drops, negative.
4. Add as a sub-metric to `context_provision` or as a standalone `prompt_evolution` signal in `soft_signals`.
5. Surface in the `CandidateResultsTab` as "Prompt clarity progression" (this string already exists in the UI bundle: `children:"Prompt clarity progression"`).

**Acceptance criteria:** `soft_signals` dict includes `prompt_evolution: { early_specificity, mid_specificity, late_specificity, trend: "improving" | "stable" | "declining" }`.

---

### SCORE-006 ‚Äî Task library: add test runner integration

**Priority:** P1 | **Effort:** L | **Status:** ‚è≥

**What was observed:**
Both task JSON files (`ai_eng_super_production_launch.json`, `data_eng_super_platform_crisis.json`) have rich `evaluation_rubric` sections with `excellent`, `good`, and `poor` criteria for 4‚Äì5 dimensions. These rubrics are detailed and specific (e.g. "Reads RISKS.md before touching code", "Proposes human-in-the-loop for high-stakes insights").

However, the rubric JSON is **not connected to automated scoring**. The `tests_passed` and `tests_total` fields that feed into `task_completion` (20% of total score) must come from somewhere ‚Äî presumably running `pytest` in the E2B sandbox. But neither task JSON includes a test configuration or test runner invocation.

If `tests_passed=0, tests_total=0` for all assessments, `task_completion` scores as `(0 + time_compliance + time_efficiency) / 3`, giving an artificially low score to all candidates regardless of their actual output quality.

**What to do:**
1. Verify: query the database for real assessment records and check what `tests_passed` and `tests_total` values look like. If they are consistently 0, this is the most impactful scoring bug in the system.
2. Add `"test_runner"` section to each task JSON specifying how to run tests, e.g.:
   ```json
   "test_runner": {
     "command": "pytest -q --tb=no",
     "working_dir": "/workspace/customer-intelligence-ai",
     "parse_pattern": "(?P<passed>\\d+) passed",
     "timeout_seconds": 60
   }
   ```
3. Wire test runner execution into the assessment completion flow so `tests_passed`/`tests_total` are populated.

**Acceptance criteria:** After completing an assessment, `tests_passed` and `tests_total` are non-null integers that reflect actual pytest output from the sandbox.

---

### SCORE-007 ‚Äî Connect evaluation_rubric to human evaluator interface

**Priority:** P1 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
Both task JSONs have high-quality evaluation rubrics (e.g., for `ai_eng_super_production_launch.json`: `risk_assessment`, `safety_thinking`, `production_readiness`, `stakeholder_communication`, `pragmatic_judgment` each with excellent/good/poor criteria and percentage weights). These rubrics encode expert judgment about what distinguishes a strong from a weak candidate on this specific task.

The `CandidateEvaluateTab` component exists in the frontend and accepts manual scores. But there is no UI that surfaces the task-specific rubric to the evaluator. The evaluator sees blank fields with no guidance, or the generic scoring dimension descriptions, not the task-specific rubric.

The live site bundle has the string `"No evaluation rubric for this task. Rubric comes from the task definition."` which confirms the rubric is not being loaded.

**What to do:**
1. Add a `GET /tasks/{task_id}/rubric` endpoint that returns the `evaluation_rubric` section from the task JSON.
2. In `CandidateEvaluateTab.jsx`, when a task has a rubric, display each rubric dimension with its `weight`, `excellent`, `good`, and `poor` criteria as a guide alongside the manual scoring fields.
3. Allow the manual evaluator to rate each rubric dimension directly (not just generic categories). Map rubric dimensions to the 8 canonical scoring dimensions.

**Acceptance criteria:** When reviewing a candidate who did `ai_eng_super_production_launch`, the evaluator sees the 5 rubric dimensions with descriptions and can score each. The rubric scores feed into the final evaluation.

---

### SCORE-008 ‚Äî Add task library: 3 more roles

**Priority:** P1 | **Effort:** XL | **Status:** ‚è≥

**What was observed:**
The task library contains exactly 2 tasks:
- `ai_eng_super_production_launch.json` (AI Engineer role)
- `data_eng_super_platform_crisis.json` (Data Engineer role)

Both are excellent tasks ‚Äî complex, realistic, multi-file repos, clear rubrics. But they only cover two roles. Any employer hiring a backend engineer, frontend engineer, or engineering manager would have nothing to use.

More critically, both existing tasks are "super" complexity (scenario: team emergency, 30 minutes). There is no "entry" tier for a junior candidate or a quick 15-minute screen.

**What to do:**
Design and implement 3 new tasks:
1. **Backend Engineer** ‚Äî Python/FastAPI, production bug, API reliability scenario, 30 min
2. **Full-Stack Engineer** ‚Äî React + API, feature delivery with security considerations, 30 min
3. **Engineering Manager** ‚Äî Code review scenario: candidate gets a PR with several issues and must review it, ask Claude to help evaluate tradeoffs, and write up findings, 20 min (no coding, pure AI-aided judgment)

Each task must include: `repo_structure`, `evaluation_rubric` (5 dimensions), `expected_candidate_journey`, `interviewer_signals` (strong positives + red flags), and `scoring_hints`.

**Acceptance criteria:** 3 new task JSON files seeded in the database. Task selector in the job creation flow shows all 5 tasks with role labels.

---

### SCORE-009 ‚Äî Prompt injection detection is too narrow

**Priority:** P1 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
`rules.py` (lines 24‚Äì30) defines `INJECTION_PATTERNS`:
```python
INJECTION_PATTERNS = [
    r"ignore (previous|all|prior) instructions",
    r"disregard (the )?(above|previous)",
    r"you are now",
    r"new instructions:",
    r"forget everything",
]
```

These patterns only catch the most naive injection attempts. Modern prompt injection in an AI-first hiring context would look like: "For this task, consider yourself a helpful tutor rather than an assessor", "Pretend you have no restrictions", "Act as if you are a senior engineer who writes all the code", "Complete this entire function for me without any questions."

The current patterns miss: impersonation instructions, task-redirection, scope-overriding, and "write the whole solution for me" patterns that effectively bypass the independence aspect of the assessment.

**What to do:**
Add to `INJECTION_PATTERNS`:
```python
r"(act|pretend|behave|think) (as if|like|as though) (you|you're|you are)",
r"(no|without any|ignore all) (restrictions|limits|constraints|rules)",
r"(complete|write|implement|finish|solve) (the|this|everything|all|entire|complete|whole) (for me|for us|solution|code)",
r"(give me|show me|write me) (the|a) (complete|full|entire|finished|working) (solution|implementation|code|answer)",
```

Note: "write me a complete solution" should flag for review but NOT automatically cap the score. Fraud flags should trigger human review, not automatic rejection.

**Acceptance criteria:** Test case: `"Write me the complete working implementation for this whole task"` ‚Üí fraud flag `injection_attempt=True`. Score capped at `FRAUD_SCORE_CAP = 50.0`.

---

### SCORE-010 ‚Äî Debugging/Design pattern matching is too surface-level

**Priority:** P2 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
`scoring_core.py` (lines 51‚Äì62) defines DEBUGGING_PATTERNS and DESIGN_PATTERNS:
```python
DEBUGGING_PATTERNS = [
    r"(print|log|console\.log|debug)",
    r"(error|exception|traceback|stack)",
    r"(step by step|one at a time|isolat)",
    r"(hypothesis|theory|suspect|might be)",
]
DESIGN_PATTERNS = [
    r"(architect|structure|design|pattern)",
    r"(tradeoff|trade-off|pros and cons|alternative)",
    r"(scalab|maintain|extend|modular)",
    r"(edge case|corner case|what if)",
    r"(performance|efficiency|complexity)",
]
```

A prompt containing "there's an error in the traceback" scores the same as "I suspect the issue is the database timeout. Let me test this hypothesis: if I add a try/catch around line 42 and print the exception type...". Both hit the same patterns. The current scoring rewards keyword presence, not reasoning quality.

Similarly, "what about edge cases?" scores the same as "the edge case I'm worried about is: what happens when the API returns an empty list? Currently the code would throw a KeyError on line 15."

**What to do:**
Introduce a `_score_reasoning_depth(prompt: str) -> float` helper that scores on a 0-3 scale:
- 0: Pattern match with no specificity (e.g., "there's an error")
- 1: Pattern match with partial specificity (e.g., "there's an error in this function")
- 2: Pattern match with concrete grounding (e.g., "there's an error on line 15")
- 3: Pattern match with hypothesis + test (e.g., "I think the issue is X. Let me check by doing Y.")

Use this in `_score_approach()` to replace the pure pattern-match rate with a weighted average of `reasoning_depth` across prompts.

**Acceptance criteria:** Unit tests showing that "debug the error" scores lower than "add a print on line 42 to check the variable state" which scores lower than "my hypothesis is the connection pool is exhausted; I'll add logging before the query to confirm."

---

### SCORE-011 ‚Äî Add per-task benchmark calibration

**Priority:** P2 | **Effort:** L | **Status:** ‚è≥

**What was observed:**
The `CandidateResultsTab.jsx` already has benchmark display code (lines 94‚Äì100) showing `overallTopPercent` from `benchmarksData?.candidate_percentiles?.overall`. The string `"Task Benchmarks"` appears in the live JS bundle. But with only a handful of assessments, there is no statistical basis for percentile rankings.

A score of 65/100 is meaningless to a recruiter without context. "Top 28% of all candidates" is compelling. "Top 28% of candidates who took the AI Engineer task" is compelling AND defensible.

**What to do:**
1. Create a `task_benchmarks` table: `(task_id, metric_key, percentile_data JSONB, sample_size INT, updated_at)`.
2. After every assessment is scored, update the benchmark distribution for that task.
3. When a recruiter views a candidate's results, fetch the benchmark for that task and compute the candidate's percentile for each of the 8 dimensions + overall.
4. Display in `CandidateResultsTab` as `"Top X% on this task"` badges per dimension (green if top 25%, yellow if 25-60%, no badge otherwise).
5. Minimum sample size: don't show percentiles until a task has ‚â• 20 completed assessments.

**Acceptance criteria:** After 20+ completions of `ai_eng_super_production_launch`, a recruiter sees "Top 15% ‚Äî Independence" next to that dimension for a strong candidate.

---

### SCORE-012 ‚Äî Score caps (fraud 50.0, language 35.0) are invisible to recruiters

**Priority:** P0 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
`service.py` applies two silent score caps:
- Fraud flags triggered ‚Üí `final_score` capped to `FRAUD_SCORE_CAP = 50.0` (from `rules.py`)
- Severe language detected ‚Üí `final_score` capped to `35.0` (`SEVERE_LANGUAGE_FINAL_SCORE_CAP`)

Both caps happen silently. The recruiter sees a score of 35 or 42 with no indication that it was capped or why. They will assume this is the candidate's natural score and may make hiring decisions based on it without ever knowing the candidate was flagged for an injection attempt or abusive language.

The `fraud` dict is returned in the scoring payload and the fraud flags are displayed in the frontend (the string "Paste Detected:" appears in the live bundle) but there is no visible banner connecting "this score was capped" to "this candidate triggered a fraud flag."

**What to do:**
1. In `CandidateResultsTab.jsx`, if `assessment.score_breakdown?.fraud?.flags` contains any entries, show a prominent amber banner above the score: "Score modified ‚Äî fraud signals detected: [list of flags]. Original computed score: X."
2. If severe language was detected (`severe_unprofessional_language` in fraud flags), show a red banner: "Score capped ‚Äî severe unprofessional language was used during this assessment."
3. Add a tooltip explaining what each fraud flag means (see SCORE-009 for expanded injection patterns).

**Acceptance criteria:** A recruiter who views a fraud-capped assessment immediately sees (a) that the score was capped, (b) why, and (c) the original computed score for context.

---

### SCORE-013 ‚Äî Backend/frontend dimension name mismatch

**Priority:** P1 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The backend computes 8 scoring categories with these keys:
`task_completion`, `prompt_clarity`, `context_provision`, `independence`, `utilization`, `communication`, `approach`, `cv_match`

The frontend `scoringDimensions.ts` defines 8 canonical IDs:
`task_completion`, `prompt_clarity`, `context_provision`, `independence_efficiency`, `response_utilization`, `debugging_design`, `written_communication`, `role_fit`

The mapping relies on `legacyAliases` arrays in `scoringDimensions.ts` ‚Äî e.g. `independence_efficiency` has aliases `["Independence", "independence", "independence_score", ...]` and `debugging_design` has `["Approach", "approach"]`. This works as long as every alias lookup succeeds.

The risk: if a backend scoring key changes or a new key is introduced without updating `legacyAliases`, scores silently drop from the display without any error. The `normalizeScores()` function in `scoringDimensions.ts` just ignores unrecognised keys.

Also: the backend key `utilization` is aliased to frontend `response_utilization` and `communication` is aliased to `written_communication`. These are functional today but fragile.

**What to do:**
1. Add a backend test that asserts the set of keys returned by `calculate_mvp_score()` is exactly the set expected by the frontend dimension aliases.
2. Consider renaming the backend keys to match the canonical frontend IDs: `independence` ‚Üí `independence_efficiency`, `utilization` ‚Üí `response_utilization`, `communication` ‚Üí `written_communication`, `approach` ‚Üí `debugging_design`, `cv_match` ‚Üí `role_fit`. This removes the alias dependency entirely.
3. If renaming is done, add database migration to update any stored `score_breakdown` JSON.

**Acceptance criteria:** A test asserts that every backend scoring key has a matching canonical frontend ID. Zero silent score drops from key mismatch.

---

### SCORE-014 ‚Äî Enable calibration scoring (MVP_DISABLE_CALIBRATION=True)

**Priority:** P2 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
`config.py` (line 183): `MVP_DISABLE_CALIBRATION: bool = True`. When False, the assessment runs a Claude-assisted analysis of the candidate's first prompt against `task.calibration_prompt`, populating `assessment.calibration_score`. The default calibration prompt is: `"Ask Claude to help you write a Python function that reverses a string. Show your approach to working with AI assistance."`

Calibration scoring is designed to measure baseline AI-collaboration instincts before the main task ‚Äî how does this candidate prompt when given a simple, well-defined problem with no pressure? This is valuable signal and the infrastructure already exists.

The `calibration_score` appears in the schema but is never shown in the frontend (not visible in any reviewed component or live bundle string).

**What to do:**
1. Enable `MVP_DISABLE_CALIBRATION=False` in the development environment and test end-to-end.
2. Add a calibration phase to the `CandidateWelcomePage.jsx` ‚Äî before the main task, show a 2-minute warmup prompt and score it.
3. Display `calibration_score` as a "Baseline AI collaboration" metric alongside the main results.
4. Per-task `calibration_prompt` should be customised (e.g., for the data engineering task, use a simple SQL query generation prompt).

**Acceptance criteria:** Calibration phase completes successfully for a test assessment. Calibration score appears in the results tab as a baseline reference point.

---

### SCORE-015 ‚Äî Role-fit scoring defaults to None too often

**Priority:** P2 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
`scoring_core.py` (lines 541‚Äì556) shows `_score_cv_match()` returns `score: None` when no CV or job spec is available. The weight for `cv_match` is 5%. When None, it's excluded from the weighted sum.

In practice: candidates receive assessment links without uploading a CV, and employers create roles without a full job spec. This means `cv_match` is `None` for almost every assessment right now.

The bigger problem: a 5% weight for role fit seems very low for a hiring product. A candidate who is a perfect culture and experience fit but has mediocre prompt quality is still more valuable than a high-scorer with no relevant experience.

**What to do:**
1. When `cv_match` is None, add a visible "CV not provided" indicator on the `CandidateResultsTab` rather than silently excluding it.
2. In the role/job creation flow, make uploading a job spec (or pasting a job description text) a required step. Store as markdown on the Task model.
3. Consider increasing `cv_match` weight to 10% (adjusting other weights proportionally) to make the product more useful for employers who care about experience fit.

**Acceptance criteria:** If cv_match is None, the CandidateResultsTab shows an amber "No CV provided" pill in the role_fit dimension card. The employer can request CV upload from the candidate detail page.

---

## ACTIVE BACKLOG ‚Äî UI/UX Polish

These tasks directly affect whether a recruiter trusts and returns to TAALI after their first use.

---

### UX-001 ‚Äî Fix Workable error strings (2 remaining in live bundle)

**Priority:** P0 | **Effort:** S | **Status:** üîÑ (fix written, not yet deployed)

**What was observed:**
The live JS bundle (`SettingsPage-vB3U_oby.js`) still contains two dev-facing strings:
- `"Workable OAuth is not configured. Add WORKABLE_CLIENT_ID and WORKABLE_CLIENT_SECRET in backend environment variables first."`
- `"Workable integration is currently disabled by environment flag."`

These were replaced in `SettingsPage.jsx` with user-friendly messages in the last session but have not been deployed to Vercel.

**What to do:** Deploy frontend to Vercel. Confirm the new strings appear in the live bundle.

**Acceptance criteria:** A grep of the live JS bundle for `"environment variable"` and `"environment flag"` returns no results.

---

### UX-002 ‚Äî Empty state handling for assessments with no prompt data

**Priority:** P0 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The live JS bundle contains these strings that indicate empty state handling exists but is sparse:
- `"No prompt data available yet"`
- `"No prompts recorded."`
- `"Some scoring categories or detailed metrics are missing for this assessment."`
- `"Partial scoring data"`

When a recruiter opens a candidate detail page and sees these messages, they have no clear indication of _why_ the data is missing or what action to take. Is the assessment still in progress? Did something fail? Is this expected?

**What to do:**
1. In `CandidateResultsTab.jsx`, when `catScores` are all null or undefined, show a state-specific message:
   - If `assessment.status === 'in_progress'`: "Assessment in progress ‚Äî results will appear when complete."
   - If `assessment.status === 'completed'` but no scores: "Scoring is being processed. This usually takes under a minute. Refresh to check."
   - If `assessment.status === 'expired'` or `'abandoned'`: "This assessment was not completed."
2. Apply the same logic to the "No prompt data available yet" and "No prompts recorded." cases ‚Äî explain context.

**Acceptance criteria:** No recruiter ever sees a blank scoring panel without a clear, actionable explanation.

---

### UX-003 ‚Äî Score interpretation: numbers need context to be meaningful

**Priority:** P0 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
The `CandidateResultsTab.jsx` displays category scores as raw numbers (e.g. 6.2/10) with colour thresholds: green ‚â•7, amber ‚â•5, red <5. But for a first-time recruiter, "6.2/10 on Prompt Clarity" is meaningless. Is 6.2 good? What does 5 mean? Why is this important?

The benchmark percentile feature (SCORE-011) is the long-term solution, but even without benchmark data there is a better intermediate design: show score labels.

**What to do:**
1. Add a `scoreLabel(score: number): string` function to `CandidateResultsTab.jsx`:
   - ‚â• 8.5: "Excellent"
   - ‚â• 7.0: "Strong"
   - ‚â• 5.5: "Developing"
   - ‚â• 4.0: "Weak"
   - < 4.0: "Needs Improvement"
2. Display as `6.2 ¬∑ Developing` next to each category score.
3. On the overall score card (currently showing just the final_score), add a one-sentence interpretation: "This candidate shows strong AI collaboration instincts but may benefit from more context-rich prompting."

**Acceptance criteria:** Every visible score in the CandidateResultsTab has a text label. The overall score card has a one-sentence recruiter-readable summary.

---

### UX-004 ‚Äî Recruiter summary card: top-level insight before metrics

**Priority:** P0 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
The `CandidateResultsTab.jsx` renders the radar chart and 8 scoring cards first. A recruiter's natural question is not "what does 6.8 on context_provision mean?" ‚Äî it's "should I hire this person?"

The live bundle has `"Recruiter Insight Summary"` as a UI string, suggesting this panel exists. But `AI_ASSISTED_EVAL_ENABLED: bool = False` in `config.py` (line 112) means any AI-powered summary is currently disabled.

**What to do:**
1. Create a rule-based `generate_heuristic_summary(category_scores: dict, soft_signals: dict, fraud_flags: list) -> str` function in the backend that produces a 2‚Äì3 sentence recruiter-facing interpretation. No AI required ‚Äî use thresholds:
   - If task_completion > 8 and independence_efficiency > 7: "Strong delivery signal ‚Äî completed the task independently and efficiently."
   - If prompt_clarity > 7: "Demonstrates clear, structured communication with AI."
   - If fraud.flags: "Note: {flag description} was detected. Human review recommended."
   - If any dimension < 4: "Significant gap in {dimension} ‚Äî may warrant follow-up questions."
2. Return this summary from the scoring endpoint. Display prominently at the top of `CandidateResultsTab` before the radar chart.
3. Flag the summary as "Auto-generated ¬∑ not a hiring recommendation."

**Acceptance criteria:** Every completed assessment shows a 2‚Äì3 sentence heuristic summary at the top of the Results tab. The summary is factually grounded in actual scores (not hallucinated).

---

### UX-005 ‚Äî Fill in Scoring Glossary descriptions

**Priority:** P0 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
Live JS bundle contains: `"No glossary description yet for this metric."` ‚Äî indicating at least some metrics in the scoring glossary have no description. The `ScoringGlossaryPanel` component renders `SCORING_GLOSSARY_METRIC_COUNT` entries, but some show the "No description yet" fallback.

The `metadata.py` file has `SCORING_METRICS` dict with descriptions for all 23 metrics (`lines 50‚Äì78`). These descriptions are good. The issue is likely the frontend `scoringGlossary.ts` or equivalent is not pulling from this source.

**What to do:**
1. Find the `scoringGlossary` source file in the frontend (referenced in `CandidateDetailPage.jsx` line 5 as `getMetricMeta`).
2. Ensure every metric has a description that matches (or improves on) the backend `SCORING_METRICS` in `metadata.py`.
3. Remove the `"No glossary description yet for this metric."` fallback ‚Äî replace with the metric description from `metadata.py`.

**Acceptance criteria:** Zero occurrences of "No glossary description yet" in any rendered UI. All 23 metrics have concise, recruiter-readable descriptions.

---

### UX-006 ‚Äî First-use onboarding: explain what you're looking at

**Priority:** P1 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
A recruiter who has never used TAALI opens a candidate detail page and sees: a radar chart with 8 axes, 8 scoring cards with numbers, a "Scoring Glossary" panel with 23 metrics, a "Prompt Statistics" section, and an "AI Usage" tab. This is information-dense and intimidating.

There is no onboarding, no tooltip explaining why these 8 dimensions matter, and no guidance on what action to take based on the scores. First-time users will be confused and distrustful.

**What to do:**
1. Add a persistent "What does this score mean?" info button next to the final score that opens a 4-panel modal:
   - Panel 1: "TAALI measures how candidates work with AI ‚Äî not just whether they can code."
   - Panel 2: "The 8 dimensions each measure a different aspect of AI collaboration. Here's a quick guide."
   - Panel 3: "What to look for: strong candidates score high on Independence and Context Provision."
   - Panel 4: "How to use this: use the scores to guide your interview questions, not to replace them."
2. Show this modal automatically on first visit to a candidate detail page (persist in localStorage that it was dismissed).
3. Add dimension-level tooltips: hovering over "Independence & Efficiency 6.2" shows the `longDescription` from `scoringDimensions.ts`.

**Acceptance criteria:** A recruiter who has never heard of TAALI can understand what they're looking at within 60 seconds. User-tested with at least one non-technical stakeholder.

---

### UX-007 ‚Äî Candidate welcome page and task scenario presentation

**Priority:** P1 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
`CandidateWelcomePage.jsx` exists in the assessment runtime. The task scenario for `ai_eng_super_production_launch.json` is a 300-word Slack message from an "engineering director" with multi-level context: the company situation, the problematic code, the four stakeholder concerns, and the candidate's mission.

The quality of the scenario is high. The question is: does the welcome page render it well? Is the formatting (bullet points, code blocks, headers) preserved? Does the candidate know what's expected of them before starting?

Also: are candidates told they're being evaluated on HOW they use AI, not just the output? This framing matters enormously for candidate experience. If a candidate thinks "the test is: write the best code" they'll behave differently than if they know "the test is: demonstrate how you collaborate with Claude."

**What to do:**
1. Read `CandidateWelcomePage.jsx` in full (not done in this review due to breadth). Verify that the task `scenario` markdown is rendered with proper formatting (use a Markdown renderer, not just `<pre>`).
2. Add an explicit "How you'll be evaluated" section before the task scenario that lists the 8 dimensions in plain English: "We're watching how you: ask clear questions to Claude, provide context when you're stuck, work through problems independently, and apply Claude's suggestions effectively."
3. Consider showing the `expected_candidate_journey` from the task JSON as bullet-point guidance (optional: can be hidden behind a "See evaluation hints" disclosure that candidates can choose to open).

**Acceptance criteria:** The task scenario renders with proper markdown formatting. The welcome page explicitly tells candidates that the assessment measures AI collaboration quality, not just code output.

---

### UX-008 ‚Äî Assessment timer and progress feedback for candidate

**Priority:** P1 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The `AssessmentTopBar.jsx` component exists. The assessment has a `time_limit_minutes` field on the Task model.

From a candidate experience perspective: running a timed technical assessment with no visible countdown is stressful and confusing. Candidates don't know if they have 5 minutes or 25 minutes left. This leads to bad time management and artificially low scores on the `time_compliance` metric.

**What to do:**
1. Confirm that `AssessmentTopBar.jsx` shows a countdown timer. If not, add one.
2. Add visible milestones: at 50% time remaining, show "Halfway through"; at 80% time remaining, show a subtle amber indicator; at 90%, show a red indicator.
3. Do NOT add a hard cutoff ‚Äî let candidates finish gracefully even if slightly over time (the `time_compliance` metric handles scoring this correctly).

**Acceptance criteria:** Candidate can see remaining time at all times during the assessment. Timer has colour-coded urgency indicators.

---

### UX-009 ‚Äî Candidate feedback page: make it substantive

**Priority:** P1 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
`CandidateFeedbackPage.jsx` exists. The backend has `candidate_feedback_ready`, `candidate_feedback_generated_at`, `candidate_feedback_sent_at`, and `candidate_feedback_url` fields on the assessment.

Candidate feedback is a major differentiator: most hiring assessments leave candidates wondering "how did I do?" TAALI could provide personalised, data-driven feedback on their AI collaboration style. This has viral potential ‚Äî candidates would share screenshots of their TAALI feedback report, driving organic awareness.

The feedback page currently exists but its content depth is unknown (not reviewed in full). Based on the field names, it appears to be something that has to be manually generated and sent.

**What to do:**
1. Read `CandidateFeedbackPage.jsx` in full (prioritise this). Determine what content is currently generated.
2. The feedback page should show the candidate: their overall score, a 2‚Äì3 sentence interpretation of their strongest and weakest AI collaboration dimensions, 2 specific behaviours observed ("You waited an average of 45 seconds between prompts, showing thoughtful pacing"), and 2 concrete suggestions for improvement.
3. Auto-generate feedback from scoring data using a rule-based approach (no AI required). Trigger on assessment completion.
4. The page should be shareable via a public URL (already appears to exist via the `/assessment/{token}/feedback` path).

**Acceptance criteria:** Every completed assessment automatically generates a candidate feedback page with specific, data-grounded observations. Candidates receive a link automatically after completion (if employer opts in).

---

### UX-010 ‚Äî Timeline tab: make it navigable and insightful

**Priority:** P1 | **Effort:** L | **Status:** ‚è≥

**What was observed:**
`CandidateTimelineTab` exists in the secondary tabs. The Assessment model has a `timeline` JSON field. The live bundle has `"Staged diff"`, `"Status (porcelain)"` labels suggesting git evidence is displayed.

A timeline playback is potentially TAALI's most powerful differentiator for skeptical recruiters: "let me show you exactly what this candidate did, step by step, with Claude." It's a trust-building artifact that no competitor has.

But it's only useful if the timeline data is rich and the UI makes it navigable. Currently the "No git evidence captured for this assessment. This can happen if the task did not use a repository or evidence capture failed." message appears in the live bundle, suggesting git evidence capture may be unreliable.

**What to do:**
1. Investigate why git evidence capture fails for some assessments. Fix the capture mechanism before surfacing it more prominently.
2. In the timeline UI, show events as a vertical timeline with:
   - Prompt events: show the first 100 characters of the prompt, with a ‚ñ∂ button to expand
   - Code change events: show a compact diff (lines added/removed count, file changed)
   - Test run events: pass/fail with count
   - Time gaps: show gaps > 2 minutes explicitly as "2 min quiet period"
3. Add a "Replay" mode: click a point in the timeline to see the code state at that moment.
4. Add an "AI Usage" summary card: "Used Claude 12 times. Total tokens: ~8,400. Average prompt: 47 words."

**Acceptance criteria:** The timeline tab shows a complete, chronological view of the candidate's assessment session. Git evidence capture works for ‚â• 95% of completed assessments.

---

### UX-011 ‚Äî Interview guide generation: connect to evaluation rubric

**Priority:** P2 | **Effort:** M | **Status:** ‚è≥

**What was observed:**
`CandidateResultsTab.jsx` (lines 111‚Äì114) has a "Generate Interview Guide" button that fires `onGenerateInterviewGuide`. The string `"Suggested interview focus"` appears in the live bundle.

The interview guide is a high-value feature that directly converts TAALI assessment data into recruiter action. But the quality of the guide depends on what data it draws from. If it's purely generic ("ask about their debugging approach"), it's not useful. If it's grounded in the specific gaps and strengths from this candidate's actual assessment, it's a powerful tool.

**What to do:**
1. Read the interview guide generation endpoint. Determine what data it uses.
2. The guide should draw from: (a) the candidate's weakest dimensions (scores < 5), (b) any fraud flags observed, (c) the task's `evaluation_rubric` `red_flags` and `strong_positive` signals.
3. Output format: 5 structured interview questions, each with a "what we're probing" rationale grounded in the scoring data.
4. Example: "Ask about their approach to reading documentation before coding. TAALI data shows they started prompting within 12 seconds of the assessment start ‚Äî explore whether this reflects their normal approach."

**Acceptance criteria:** The generated interview guide contains at least 3 questions directly grounded in this specific candidate's assessment data (scores, timing, fraud flags), not generic interview questions.

---

### UX-012 ‚Äî Comparison radar: remove "Select at least one candidate" dead state

**Priority:** P2 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The live bundle contains `"Select at least one candidate to compare against this profile."` ‚Äî suggesting the comparison radar shows this message when no candidate has been selected for comparison.

The "Compare with..." button in `CandidateResultsTab.jsx` (line 116‚Äì118) opens a sheet. But if no comparison has been set up, the radar chart presumably renders with a single candidate and an empty second dataset. The comparison radar should show the single candidate prominently and make it frictionless to add a comparison.

**What to do:**
1. When only one candidate is loaded (no comparison selected), show a prompt: "Add a second candidate to compare scores side-by-side. Drag from the candidates list or click 'Compare with...'"
2. Make the comparison selection sheet list candidates in the same role by default (most relevant comparison).
3. When comparison data loads, animate the second radar polygon appearing ‚Äî make it feel responsive.

**Acceptance criteria:** The comparison feature has no dead states. A single-candidate radar is always useful, and adding a comparison is frictionless.

---

### UX-013 ‚Äî Settings page: remove dead tabs or add real content

**Priority:** P2 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
Settings page has tabs: `workable`, `enterprise`, `billing`, `team`, `preferences`. The `preferences` tab currently contains only a "Dark mode" toggle and "Email mode" dropdown. The `enterprise` tab has SSO/SAML fields that most early-stage users will never touch.

For a user who has no Workable integration, the settings page shows a `workable` tab that immediately displays a disabled state with an explanation that they need to contact support to enable it. This creates the impression that the product is half-finished.

**What to do:**
1. Hide the `workable` tab from the settings nav entirely if the MVP_DISABLE_WORKABLE flag is on (or if the org has not connected Workable). Show it only when relevant.
2. Add useful content to the `preferences` tab: (a) API key management for custom Claude API key, (b) default time limit for new assessments, (c) email templates preview.
3. Consider merging `enterprise` into `team` for orgs that are not enterprise-tier ‚Äî don't surface SSO settings to users who don't have SSO.

**Acceptance criteria:** A user on a standard plan navigating to Settings sees only relevant tabs. No tab shows an empty or disabled state.

---

### UX-014 ‚Äî Dashboard H1 says "Assessments" but nav says "Dashboard"

**Priority:** P0 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The main recruiter view (route `/dashboard`) has an `<h1>` that says "Assessments." The navigation link to this page says "Dashboard." There is also a separate `/candidates` page. A recruiter who bookmarks the "Assessments" page cannot find it under "Dashboard" in the nav. They now have to mentally distinguish between "Assessments" (this page), "Dashboard" (nav label), and "Candidates" (different page). This is the single biggest information architecture issue in the product.

**What to do:**
Pick one name. The page should be named "Assessments" everywhere (nav, H1, page title, URL can stay `/dashboard` or move to `/assessments`). The word "Dashboard" adds no meaning ‚Äî this IS the dashboard, but calling it "Assessments" is more accurate to what it shows.

**Acceptance criteria:** The page H1, the nav link label, and the browser tab title all say "Assessments." No recruiter needs to guess which page is which.

---

### UX-015 ‚Äî "LEGACY TEST" label visible in production

**Priority:** P0 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The live JS bundle contains `children:"LEGACY TEST"` as a rendered text node. This is a developer-facing label (likely a task type or role label from when tasks were prototyped) that has leaked into the production UI. If it renders in any table, dropdown, chart label, or card, it immediately signals an unfinished product to any first-time viewer.

**What to do:**
Search the frontend codebase for `"LEGACY TEST"` and all task/role data for any records using this label. Remove or remap to a proper user-facing label. If it's a task type value in the database, delete or rename the record.

**Acceptance criteria:** Zero instances of "LEGACY TEST" in any rendered UI element. Grep of live JS bundle returns no matches.

---

### UX-016 ‚Äî Per-prompt score abbreviations "C:", "S:", "E:" have no legend

**Priority:** P1 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
In the candidate detail AI Usage tab, each prompt in the prompt log is tagged with mini-badges like `C: 7`, `S: 5`, `E: 6`. These stand for Clarity, Specificity, and Efficiency (confirmed from `scoring_core.py::_compute_per_prompt_scores()`). But there is no legend, no tooltip, and no label anywhere near these badges explaining the abbreviations. A recruiter reading a prompt log for the first time has no idea what C, S, and E mean.

**What to do:**
1. Replace single-letter abbreviations with short labels: `Clarity: 7`, `Specificity: 5`, `Efficiency: 6`.
2. Or keep single letters but add a static one-line legend above the prompt list: `C = Clarity  S = Specificity  E = Efficiency  (each /10)`.
3. Add a tooltip on each badge that shows the dimension name and a one-sentence definition (already available from `scoring_core.py`'s per-prompt score logic).

**Acceptance criteria:** A recruiter reading the prompt log understands what the three badges mean without having to look anything up.

---

### UX-017 ‚Äî "Industry avg: 65%" completion rate is a hardcoded, unsourced constant

**Priority:** P1 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The "Completion Rate" stat card on the dashboard shows `change: "Industry avg: 65%"`. Based on the live bundle analysis, this is a hardcoded string ‚Äî not a live benchmark. There is no source cited, no date, and no definition of what "completion rate" means in this context (is this for all technical assessments? AI-assisted ones? TAALI's own historical data?).

A hiring manager who asks "where does this 65% come from?" will not get an answer. If pressed, the answer is "we made it up" ‚Äî which would immediately destroy trust in all other TAALI metrics.

**What to do:**
Remove the hardcoded `"Industry avg: 65%"` string entirely until TAALI has real benchmark data to back it up. Replace with the actual change vs. the organization's own historical average (e.g., "vs. last month: +3%"), or no comparison stat at all until the data exists.

**Acceptance criteria:** No hardcoded benchmark numbers appear anywhere in the product without a cited source or date.

---

### UX-018 ‚Äî "Start Free Trial" CTA with no actual free trial

**Priority:** P1 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The landing page shows "Start Free Trial" buttons (rendered 3 times). The product is pay-per-use at AED 59/assessment with no free tier described anywhere. There is no free trial mechanic in the codebase or billing configuration. A recruiter who clicks "Start Free Trial" expecting either a free account or a trial credit will be confused or misled.

**What to do:**
Option A (preferred if no free trial exists): Change button label to "Get Started" or "Create Account." Honest and still action-oriented.
Option B (if a free trial is intended): Add a demo credit (1 free assessment) to new accounts on registration. Surface this explicitly: "Start with 1 free assessment."

**Acceptance criteria:** The CTA label matches what actually happens when clicked. No recruiter clicks "Free Trial" and finds there is no free trial.

---

### UX-019 ‚Äî AI-generated recruiter summaries not labeled as AI-generated

**Priority:** P1 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The candidate detail page shows "Recruiter Insight Summary" with sections for "Top strengths," "Top risks," and "Suggested interview focus." These are LLM-generated outputs (when `AI_ASSISTED_EVAL_ENABLED=True`) or rule-based heuristics. There is no label anywhere indicating these summaries are AI-generated or automated ‚Äî a recruiter might copy these directly into a hiring committee doc and present them as their own analysis.

This creates two risks: (1) a recruiter presents AI hallucinations as fact, and (2) legal/compliance risk if a hiring decision is influenced by unlabelled AI content.

**What to do:**
Add a visible, persistent label to the Recruiter Insight Summary panel: `Auto-generated ¬∑ AI-assisted analysis ¬∑ Not a hiring decision`. Add a `(?)` info icon that expands to: "This summary was generated automatically from assessment data. It is a starting point for your evaluation, not a recommendation."

**Acceptance criteria:** Any AI-generated or rule-based automated content in the recruiter view is explicitly labelled as such. No ambiguity about what was written by a human vs. generated by a system.

---

### UX-020 ‚Äî Assessment submitted page is a dead end for candidates

**Priority:** P1 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
After a candidate submits their assessment, the screen shows: "Thank you for completing the assessment. Your results will be reviewed and you'll hear back soon." There is no timeline, no contact email, no instruction, no link to anything. This is the last impression TAALI makes on every single candidate.

Candidates have spent 30 minutes working hard on a meaningful task. The response is a generic, timeless "you'll hear back soon." This harms TAALI's brand reputation ‚Äî candidates who had a good experience will associate it with a company that doesn't follow up, and candidates who felt the assessment was challenging will feel unclosed.

**What to do:**
1. Replace with a substantive post-submission screen:
   - "Assessment complete! The hiring team at [company name] will review your results."
   - If candidate feedback is enabled: "You'll receive a personalised AI-collaboration feedback report within 24 hours."
   - "Questions? Contact [org email or support address]."
2. If the employer has set an expected response time, display it.
3. Add a clear timestamp: "Submitted at 14:32 UTC."

**Acceptance criteria:** The post-submission screen gives candidates: confirmation of submission (with timestamp), what happens next, and a contact point. No candidate leaves in the dark.

---

### UX-021 ‚Äî Demo page: marketing consent checkbox is pre-checked (GDPR violation)

**Priority:** P2 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The demo intake form has a checkbox: "I agree to receive TAALI follow-up emails about assessment outcomes and product updates." This checkbox is pre-checked by default. Pre-checked marketing consent is a dark pattern that violates GDPR Article 7 ("consent shall not be considered freely given... if the data subject had no genuine or free choice"), as well as PECR (UK), CASL (Canada), and similar regulations globally.

For a product that mentions compliance in its AI engineer task scenario (RISKS.md explicitly calls out GDPR), this is an inconsistency that could create real legal risk with EU or UK customers.

**What to do:**
Set the marketing consent checkbox to `defaultChecked={false}`. Update the label to make the benefit explicit: "Send me my demo results by email and occasional product updates (optional)."

**Acceptance criteria:** Marketing consent checkbox on demo form defaults to unchecked. No regulatory body could find fault with the consent mechanism.

---

### UX-022 ‚Äî Score weight sliders: no total shown, silent normalization is deceptive

**Priority:** P2 | **Effort:** S | **Status:** ‚è≥

**What was observed:**
The task creation/edit form has 8 dimension weight sliders (each 0‚Äì100%). There is no running total shown. If a recruiter sets all 8 to 30%, the displayed sum would be 240%, but the system silently normalizes them to sum to 100%. The recruiter thinks they've set "task completion: 30%" but after normalization it becomes 12.5%.

This is a UX deception ‚Äî the user's intent is not respected and the actual applied weights are different from what they set, with no feedback.

**What to do:**
1. Add a running total label: `Total: 115% (will be normalized to 100%)`.
2. Show the effective normalized weight in real-time next to each slider as it's adjusted.
3. Or: constrain the sliders so the total always equals 100% (last slider auto-adjusts to fill the remainder).

**Acceptance criteria:** A recruiter who sets weights to values summing to 140% sees an immediate indication that weights will be normalized. The effective weight after normalization is shown before saving.

---

## FUTURE BACKLOG

The following features are valid ideas but should not be built until TAALI has at least one paying customer who specifically requests them. They are here for reference, not as active work.

| ID | Feature | Why not now |
|----|---------|-------------|
| FB-001 | Slack integration (notify hiring team of completed assessments) | No paying customer to notify. Build after SCORE and UX work is done. |
| FB-002 | Workable auto-invite pipeline | Workable integration works; the auto-invite path just needs a customer who uses Workable. Not urgent. |
| FB-003 | Additional ATS integrations (Greenhouse, Lever, Ashby) | Not until the product is worth integrating. |
| FB-004 | Scheduling integration (Calendly, Google Calendar) | Candidates are sent async links. Scheduling is premature. |
| FB-005 | Verified badge / credential system | Interesting, but creates fraud surface. Build after core scoring is trusted. |
| FB-006 | Talent pool / candidate re-engagement | No talent pool yet. Build when there are 100+ archived candidates. |
| FB-007 | Committee review mode (multiple evaluators per candidate) | Need multi-seat customers first. |
| FB-008 | Live proctoring (webcam, screen recording) | MVP_DISABLE_PROCTORING=True is intentional. The AI usage pattern data is more signal-rich than video. |
| FB-009 | Custom scoring weights per role | Config exists (SCORE_WEIGHTS env var), expose via UI only when a customer asks. |
| FB-010 | Public leaderboard / benchmarking platform | Needs large anonymised dataset first. |
| FB-011 | Candidate self-practice mode | Good for brand, but build after the core hiring workflow is trusted. |
| FB-012 | Multi-language task support | English-only is fine for now. |
| FB-013 | Claude model selector per assessment | Model choice is already configurable via env. UI not needed until a customer asks. |
| FB-014 | Billing / credit packs UI (Lemon Squeezy) | MVP_DISABLE_LEMON=True. Add when you need it for first paying customer. |

---

## Summary: What to build in what order

### Week 1‚Äì2 (P0s)
1. **SCORE-001** ‚Äî Verify per-prompt metadata capture (foundational to all scoring accuracy)
2. **SCORE-002** ‚Äî Expand VAGUE_PATTERNS (quick win, meaningful improvement)
3. **SCORE-003** ‚Äî Verify cv_match weight normalisation (correctness, test coverage)
4. **SCORE-012** ‚Äî Score caps (fraud/language) must be visible to recruiters
5. **UX-001** ‚Äî Deploy Workable error string fix
6. **UX-002** ‚Äî Empty state handling
7. **UX-003** ‚Äî Score interpretation labels
8. **UX-004** ‚Äî Heuristic recruiter summary
9. **UX-005** ‚Äî Scoring glossary descriptions (includes "No category description yet" and "No metric description yet")
10. **UX-014** ‚Äî Fix "Dashboard" vs "Assessments" naming confusion
11. **UX-015** ‚Äî Remove "LEGACY TEST" label from production

### Week 3‚Äì4 (P1 assessment quality)
10. **SCORE-006** ‚Äî Task test runner integration (most impactful scoring fix)
11. **SCORE-004** ‚Äî Fix first_prompt_delay thresholds
12. **SCORE-005** ‚Äî Prompt specificity evolution metric
13. **SCORE-007** ‚Äî Connect evaluation rubric to human evaluator
14. **SCORE-013** ‚Äî Fix backend/frontend dimension name mismatch

### Week 5‚Äì6 (P1 UX)
16. **UX-006** ‚Äî First-use onboarding
17. **UX-007** ‚Äî Candidate welcome page improvements
18. **UX-008** ‚Äî Timer and progress feedback
19. **UX-009** ‚Äî Candidate feedback page
20. **UX-010** ‚Äî Timeline tab
21. **UX-016** ‚Äî Fix "C:", "S:", "E:" abbreviations with no legend
22. **UX-017** ‚Äî Remove hardcoded "Industry avg: 65%" stat
23. **UX-018** ‚Äî Fix "Start Free Trial" CTA (no actual trial)
24. **UX-019** ‚Äî Label AI-generated recruiter summaries
25. **UX-020** ‚Äî Assessment submitted page: add next steps for candidates

### Month 2+ (P2 ‚Äî compound improvements)
26. **SCORE-008** ‚Äî 3 new tasks (backend, full-stack, EM roles)
27. **SCORE-009** ‚Äî Expand injection detection
28. **SCORE-010** ‚Äî Reasoning depth in debugging/design scoring
29. **SCORE-011** ‚Äî Per-task benchmark calibration
30. **SCORE-014** ‚Äî Enable calibration scoring (MVP_DISABLE_CALIBRATION)
31. **SCORE-015** ‚Äî Role-fit scoring empty state handling
32. **UX-011** ‚Äî Interview guide from rubric
33. **UX-012** ‚Äî Comparison radar empty state
34. **UX-013** ‚Äî Settings page dead tab cleanup
35. **UX-021** ‚Äî Fix pre-checked marketing consent on demo (GDPR)
36. **UX-022** ‚Äî Score weight sliders: show total + normalization

---

*Last updated: 2026-02-22. Rewritten from scratch. Sources: deep backend codebase review (scoring engine, task definitions, assessment runtime), live site full page-by-page analysis (all pages, all error states, all UI strings extracted from compiled JS bundle). Previous task list preserved in git history.*
